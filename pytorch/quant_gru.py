"""
QuantGRU - æ”¯æŒé‡åŒ–çš„ GRU å®ç°

åŠŸèƒ½ç‰¹æ€§:
    - å…¼å®¹ nn.GRU æ¥å£(æ”¯æŒ batch_firstã€bidirectional ç­‰å‚æ•°)
    - æ”¯æŒ INT8/INT16/INT32 æ··åˆç²¾åº¦é‡åŒ–æ¨ç†
    - æ”¯æŒ MinMax / SQNR / Percentile æ ¡å‡†æ–¹æ³•
    - æ”¯æŒ JSON é…ç½®æ–‡ä»¶æŒ‡å®šå„ç®—å­çš„ä½å®½å’Œå¯¹ç§°é‡åŒ–è®¾ç½®
    - æ”¯æŒ ONNX å¯¼å‡º(float / QDQ ä¸¤ç§æ ¼å¼)

å…³é”®å±æ€§:
    - use_quantization: æ˜¯å¦å¯ç”¨é‡åŒ–(é»˜è®¤ False)
    - calibrating: æ˜¯å¦åœ¨ forward ä¸­æ”¶é›†æ ¡å‡†æ•°æ®(é»˜è®¤ False)
    - calibration_method: æ ¡å‡†æ–¹æ³• 'minmax'|'sqnr'|'percentile'(é»˜è®¤ 'sqnr')
    - export_mode: æ˜¯å¦ä½¿ç”¨ ONNX å¯¼å‡ºæ¨¡å¼(é»˜è®¤ False)
    - export_format: ONNX å¯¼å‡ºæ ¼å¼ 'float'|'qdq'(é»˜è®¤ 'float')

å…¸å‹ç”¨æ³•:
    >>> from quant_gru import QuantGRU
    >>>
    >>> # åˆ›å»ºæ¨¡å‹
    >>> gru = QuantGRU(64, 128, batch_first=True).cuda()
    >>>
    >>> # åŠ è½½ä½å®½é…ç½®(å¯é€‰)
    >>> gru.load_bitwidth_config("config.json", verbose=True)
    >>>
    >>> # æ ¡å‡†(åœ¨ forward ä¸­æ”¶é›†æ ¡å‡†æ•°æ®)
    >>> gru.calibrating = True
    >>> output = gru(calibration_data)  # åŒæ—¶è¿”å›è¾“å‡ºå¹¶æ”¶é›†æ ¡å‡†æ•°æ®
    >>> gru.calibrating = False
    >>>
    >>> # é‡åŒ–æ¨ç†(è‡ªåŠ¨è°ƒç”¨ finalize_calibration)
    >>> gru.use_quantization = True
    >>> output = gru(x)

ONNX å¯¼å‡º:
    >>> gru.export_mode = True
    >>> gru.export_format = 'float'  # æˆ– 'qdq' (å¯é€‰)
    >>> torch.onnx.export(gru, x, "model.onnx")
    >>> gru.export_mode = False
"""

import json
import torch
import torch.nn as nn
from typing import Optional, Tuple

try:
    import gru_interface_binding as gru_ops
except ImportError:
    raise ImportError(
        "gru_interface_binding æ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ setup.py ç¼–è¯‘ C++ æ‰©å±•"
    )

# ============================================================
#                   æ¨¡å—çº§å¸¸é‡ä¸é…ç½®æ˜ å°„
# ============================================================
#
# æœ¬èŠ‚å®šä¹‰ JSON é…ç½®æ–‡ä»¶ä¸ C++ OperatorQuantConfig ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚
# é‡‡ç”¨ 2 å±‚è®¾è®¡ï¼šJSON æ–‡ä»¶ â†’ C++ å¯¹è±¡ï¼ˆæ— ä¸­é—´ Python å­—å…¸å±‚ï¼‰ã€‚
#
# JSON é…ç½®ç¤ºä¾‹:
#   {
#     "GRU_config": {
#       "operator_config": {
#         "input.x": {"bitwidth": 8, "is_symmetric": true},
#         ...
#       }
#     }
#   }

# JSON é…ç½®å­—æ®µæ˜ å°„è¡¨
# æ ¼å¼: "JSONé”®å" -> ("C++ä½å®½å±æ€§å", "C++å¯¹ç§°é‡åŒ–å±æ€§å")
_BITWIDTH_FIELD_MAP = {
    "input.x": ("x_", "x_symmetric_"),
    "input.h": ("h_", "h_symmetric_"),
    "weight.W": ("W_", "W_symmetric_"),
    "weight.R": ("R_", "R_symmetric_"),
    "weight.bx": ("bx_", "bx_symmetric_"),
    "weight.br": ("br_", "br_symmetric_"),
    "matmul.Wx": ("Wx_", "Wx_symmetric_"),
    "matmul.Rh": ("Rh_", "Rh_symmetric_"),
    "gate.z_pre": ("z_pre_", "z_pre_symmetric_"),
    "gate.z_out": ("z_out_", "z_out_symmetric_"),
    "gate.r_pre": ("r_pre_", "r_pre_symmetric_"),
    "gate.r_out": ("r_out_", "r_out_symmetric_"),
    "gate.g_pre": ("g_pre_", "g_pre_symmetric_"),
    "gate.g_out": ("g_out_", "g_out_symmetric_"),
    "op.Rh_add_br": ("Rh_add_br_", "Rh_add_br_symmetric_"),
    "op.rRh": ("rRh_", "rRh_symmetric_"),
    "op.old_contrib": ("old_contrib_", "old_contrib_symmetric_"),
    "op.new_contrib": ("new_contrib_", "new_contrib_symmetric_"),
}

# æ´¾ç”Ÿå¸¸é‡ï¼šä»æ˜ å°„è¡¨æå–çš„ C++ å±æ€§åé›†åˆ
_VALID_BITWIDTH_ATTRS = {bw_attr for bw_attr, _ in _BITWIDTH_FIELD_MAP.values()}  # 18 ä¸ªä½å®½å±æ€§
_VALID_SYMMETRIC_ATTRS = {sym_attr for _, sym_attr in _BITWIDTH_FIELD_MAP.values()}  # 18 ä¸ªå¯¹ç§°å±æ€§

# å¯¹ç§°é‡åŒ–å±æ€§åˆ†ç±»ï¼ˆç”¨äº set_all_bitwidthï¼‰
# - æƒé‡/åç½®ï¼šå§‹ç»ˆä½¿ç”¨å¯¹ç§°é‡åŒ–ï¼ˆzero_point=0ï¼‰ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜
# - æ¿€æ´»å€¼ï¼šå¯é…ç½®ï¼Œéå¯¹ç§°é‡åŒ–å¯èƒ½æé«˜ç²¾åº¦ä½†å¢åŠ è®¡ç®—å¼€é”€
_WEIGHT_SYMMETRIC_ATTRS = {'W_symmetric_', 'R_symmetric_', 'bx_symmetric_', 'br_symmetric_'}
_ACTIVATION_SYMMETRIC_ATTRS = _VALID_SYMMETRIC_ATTRS - _WEIGHT_SYMMETRIC_ATTRS


def _validate_bitwidth_field_map():
    """
    éªŒè¯ Python ç«¯æ˜ å°„è¡¨ä¸ C++ ç«¯å±æ€§å®šä¹‰ä¸€è‡´æ€§
    
    åœ¨æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨è°ƒç”¨ï¼Œç¡®ä¿ _BITWIDTH_FIELD_MAP ä¸­çš„å±æ€§å
    ä¸ C++ OperatorQuantConfig çš„å®é™…å±æ€§ä¸€è‡´ã€‚
    ä¸ä¸€è‡´æ—¶ç«‹å³æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…è¿è¡Œæ—¶é™é»˜å¤±è´¥ã€‚
    """
    try:
        test_config = gru_ops.OperatorQuantConfig()
    except Exception as e:
        # gru_ops å¯èƒ½æœªæ­£ç¡®åŠ è½½ï¼Œè·³è¿‡éªŒè¯ï¼ˆåç»­ä½¿ç”¨æ—¶ä¼šæŠ¥é”™ï¼‰
        import warnings
        warnings.warn(f"æ— æ³•éªŒè¯ _BITWIDTH_FIELD_MAP: {e}")
        return

    missing_attrs = []
    for json_key, (bw_attr, sym_attr) in _BITWIDTH_FIELD_MAP.items():
        if not hasattr(test_config, bw_attr):
            missing_attrs.append(f"{json_key} -> {bw_attr}")
        if not hasattr(test_config, sym_attr):
            missing_attrs.append(f"{json_key} -> {sym_attr}")

    if missing_attrs:
        raise RuntimeError(
            f"_BITWIDTH_FIELD_MAP ä¸ C++ OperatorQuantConfig ä¸ä¸€è‡´ï¼\n"
            f"ç¼ºå°‘å±æ€§: {missing_attrs}\n"
            f"è¯·æ£€æŸ¥ gru_interface_binding.cc ä¸­çš„ OperatorQuantConfigPy å®šä¹‰"
        )


# æ¨¡å—åŠ è½½æ—¶æ‰§è¡Œä¸€è‡´æ€§éªŒè¯ï¼ˆimport æ—¶è‡ªåŠ¨è¿è¡Œï¼‰
_validate_bitwidth_field_map()


# ============================================================
#                      æ ¼å¼åŒ–è¾…åŠ©å‡½æ•°
# ============================================================

def _format_bitwidth(val: int) -> str:
    """æ ¼å¼åŒ–ä½å®½å€¼: 8 -> '8bit'"""
    return f"{abs(val)}bit"


def _format_symmetric(is_symmetric: bool) -> str:
    """æ ¼å¼åŒ–å¯¹ç§°é‡åŒ–: True -> 'å¯¹ç§°'"""
    return "å¯¹ç§°" if is_symmetric else "éå¯¹ç§°"


def print_bitwidth_config(config: gru_ops.OperatorQuantConfig,
                          config_file: str = None,
                          verbose: bool = True) -> None:
    """
    æ‰“å° OperatorQuantConfig çš„é…ç½®è¯¦æƒ…
    
    Args:
        config: é…ç½®å¯¹è±¡
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„(å¯é€‰ï¼Œä»…ç”¨äºæ˜¾ç¤ºæ¥æº)
        verbose: æ˜¯å¦æ‰“å°è¯¦æƒ…(é»˜è®¤ True)
    """
    if not verbose:
        return

    print("\n" + "=" * 70)
    print("ğŸ”§ GRU é‡åŒ–é…ç½®(ä½å®½ + å¯¹ç§°é‡åŒ–)")
    print("=" * 70)
    if config_file:
        print(f"ğŸ“„ é…ç½®æ¥æº: {config_file}")
    print("-" * 70)
    print(f"  [è¾“å…¥]  x: {_format_bitwidth(config.x_):6s} ({_format_symmetric(config.x_symmetric_)})")
    print(f"          h: {_format_bitwidth(config.h_):6s} ({_format_symmetric(config.h_symmetric_)})")
    print(f"  [æƒé‡]  W: {_format_bitwidth(config.W_):6s} ({_format_symmetric(config.W_symmetric_)})")
    print(f"          R: {_format_bitwidth(config.R_):6s} ({_format_symmetric(config.R_symmetric_)})")
    print(f"          bx: {_format_bitwidth(config.bx_):6s} ({_format_symmetric(config.bx_symmetric_)})")
    print(f"          br: {_format_bitwidth(config.br_):6s} ({_format_symmetric(config.br_symmetric_)})")
    print(f"  [çŸ©é˜µ]  Wx: {_format_bitwidth(config.Wx_):6s} ({_format_symmetric(config.Wx_symmetric_)})")
    print(f"          Rh: {_format_bitwidth(config.Rh_):6s} ({_format_symmetric(config.Rh_symmetric_)})")
    print(f"  [é—¨æ§]  z_pre: {_format_bitwidth(config.z_pre_):6s} ({_format_symmetric(config.z_pre_symmetric_)})")
    print(f"          z_out: {_format_bitwidth(config.z_out_):6s} ({_format_symmetric(config.z_out_symmetric_)})")
    print(f"          r_pre: {_format_bitwidth(config.r_pre_):6s} ({_format_symmetric(config.r_pre_symmetric_)})")
    print(f"          r_out: {_format_bitwidth(config.r_out_):6s} ({_format_symmetric(config.r_out_symmetric_)})")
    print(f"          g_pre: {_format_bitwidth(config.g_pre_):6s} ({_format_symmetric(config.g_pre_symmetric_)})")
    print(f"          g_out: {_format_bitwidth(config.g_out_):6s} ({_format_symmetric(config.g_out_symmetric_)})")
    print(
        f"  [è¿ç®—]  Rh+br: {_format_bitwidth(config.Rh_add_br_):6s} ({_format_symmetric(config.Rh_add_br_symmetric_)})")
    print(f"          rRh: {_format_bitwidth(config.rRh_):6s} ({_format_symmetric(config.rRh_symmetric_)})")
    print(
        f"  [è¾“å‡º]  old: {_format_bitwidth(config.old_contrib_):6s} ({_format_symmetric(config.old_contrib_symmetric_)})")
    print(
        f"          new: {_format_bitwidth(config.new_contrib_):6s} ({_format_symmetric(config.new_contrib_symmetric_)})")
    print("=" * 70 + "\n")


# ============================================================
#                      æƒé‡æ ¼å¼è½¬æ¢
# ============================================================
#
# PyTorch GRU å’Œ Haste GRU ä½¿ç”¨ä¸åŒçš„é—¨æ§é¡ºåºï¼š
#   - PyTorch: (reset, update, new) å³ (r, z, n)
#   - Haste:   (update, reset, new) å³ (z, r, n)
#
# æƒé‡å¼ é‡å½¢çŠ¶ä¸º [3*H, ...]ï¼Œéœ€è¦é‡æ’åºå‰ 2/3 çš„éƒ¨åˆ†ã€‚

def reorder_weights_pytorch_to_haste(w: torch.Tensor) -> torch.Tensor:
    """
    PyTorch æƒé‡æ ¼å¼ (r,z,n) -> Haste æ ¼å¼ (z,r,n)
    
    Args:
        w: å½¢çŠ¶ [3*H, ...] çš„æƒé‡å¼ é‡
        
    Returns:
        é‡æ’åºåçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸å˜
    """
    w = w.contiguous()
    h3 = w.shape[0] // 3
    device = w.device
    # [r, z, n] -> [z, r, n]
    indices = torch.cat([
        torch.arange(h3, 2 * h3, device=device),
        torch.arange(0, h3, device=device),
        torch.arange(2 * h3, 3 * h3, device=device)
    ])
    return w.index_select(0, indices).contiguous()


def reorder_weights_haste_to_pytorch(w: torch.Tensor) -> torch.Tensor:
    """
    Haste æƒé‡æ ¼å¼ (z,r,n) -> PyTorch æ ¼å¼ (r,z,n)
    
    Args:
        w: å½¢çŠ¶ [3*H, ...] çš„æƒé‡å¼ é‡
        
    Returns:
        é‡æ’åºåçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸å˜
    """
    w = w.contiguous()
    h3 = w.shape[0] // 3
    device = w.device
    # [z, r, n] -> [r, z, n]
    indices = torch.cat([
        torch.arange(h3, 2 * h3, device=device),
        torch.arange(0, h3, device=device),
        torch.arange(2 * h3, 3 * h3, device=device)
    ])
    return w.index_select(0, indices).contiguous()


def ensure_cuda_float32(tensor: torch.Tensor, device: torch.device) -> torch.Tensor:
    """ç¡®ä¿å¼ é‡åœ¨ CUDA ä¸Šä¸”ä¸º float32 ç±»å‹"""
    if not tensor.is_cuda:
        tensor = tensor.to(device)
    if tensor.dtype != torch.float32:
        tensor = tensor.float()
    return tensor


def convert_weights_to_haste_format(
        weight_ih: torch.Tensor,
        weight_hh: torch.Tensor,
        bias_ih: Optional[torch.Tensor],
        bias_hh: Optional[torch.Tensor],
        hidden_size: int,
        device: torch.device
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    å°† PyTorch GRU æƒé‡è½¬æ¢ä¸º Haste æ ¼å¼(ç‹¬ç«‹å·¥å…·å‡½æ•°)
    
    PyTorch æ ¼å¼: (r, z, n)
    Haste æ ¼å¼:   (z, r, n)
    
    Args:
        weight_ih: [3*H, I] è¾“å…¥æƒé‡ (PyTorch æ ¼å¼)
        weight_hh: [3*H, H] å¾ªç¯æƒé‡ (PyTorch æ ¼å¼)
        bias_ih: [3*H] è¾“å…¥åç½® æˆ– None
        bias_hh: [3*H] å¾ªç¯åç½® æˆ– None
        hidden_size: éšè—å±‚å¤§å°(ç”¨äºåˆ›å»ºé›¶åç½®)
        device: ç›®æ ‡è®¾å¤‡
        
    Returns:
        (W, R, bx, br): Haste æ ¼å¼çš„æƒé‡å’Œåç½®
            - W: [I, 3*H] è½¬ç½®åçš„è¾“å…¥æƒé‡
            - R: [H, 3*H] è½¬ç½®åçš„å¾ªç¯æƒé‡
            - bx: [3*H] è¾“å…¥åç½®
            - br: [3*H] å¾ªç¯åç½®
    """
    # æƒé‡è½¬æ¢: é‡æ’åº (r,z,n) -> (z,r,n) å¹¶è½¬ç½®
    weight_ih = ensure_cuda_float32(weight_ih, device)
    weight_hh = ensure_cuda_float32(weight_hh, device)
    W = reorder_weights_pytorch_to_haste(weight_ih).t().contiguous()
    R = reorder_weights_pytorch_to_haste(weight_hh).t().contiguous()

    # åç½®å¤„ç†
    if bias_ih is not None and bias_hh is not None:
        bias_ih = ensure_cuda_float32(bias_ih, device)
        bias_hh = ensure_cuda_float32(bias_hh, device)
        bx = reorder_weights_pytorch_to_haste(bias_ih).contiguous()
        br = reorder_weights_pytorch_to_haste(bias_hh).contiguous()
    else:
        bx = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)
        br = torch.zeros(3 * hidden_size, device=device, dtype=torch.float32)

    return W, R, bx, br


# ============================================================
#                      QDQ (Quantize-Dequantize) ä¼ªé‡åŒ–
# ============================================================
#
# ä¼ªé‡åŒ–ç”¨äº ONNX å¯¼å‡ºï¼Œåœ¨æµ®ç‚¹åŸŸæ¨¡æ‹Ÿé‡åŒ–æ•ˆæœï¼š
#   q = clamp(round(x / scale) + zp, qmin, qmax)
#   x' = (q - zp) * scale
#
# æ¨ç†å¼•æ“ï¼ˆå¦‚ TensorRTï¼‰ä¼šè¯†åˆ« QDQ æ¨¡å¼å¹¶æ›¿æ¢ä¸ºçœŸå®é‡åŒ–ç®—å­ã€‚
#
# é‡åŒ–å‚æ•°è¯´æ˜ï¼š
#   - exp2_inv: é‡åŒ–æŒ‡æ•°ï¼Œscale = 2^(-exp2_inv)
#   - zp: é›¶ç‚¹ï¼ˆå¯¹ç§°é‡åŒ–æ—¶ä¸º 0ï¼‰
#   - bitwidth: ä½å®½ (8/16/32)

def fake_quantize(x: torch.Tensor, exp2_inv: int, zp: int = 0,
                  bitwidth: int = 8, symmetric: bool = True,
                  is_unsigned: bool = False) -> torch.Tensor:
    """
    ä¼ªé‡åŒ–(Fake Quantize): é‡åŒ–åç«‹å³åé‡åŒ–ï¼Œä¿æŒæµ®ç‚¹æ ¼å¼
    
    ç”¨äº ONNX å¯¼å‡ºï¼Œæ¨ç†å¼•æ“ä¼šè¯†åˆ« QDQ æ¨¡å¼å¹¶ä¼˜åŒ–
    
    [ä¸ CUDA ä¸€è‡´] é‡åŒ–å‚æ•° (exp2_inv, zp) ä¸ CUDA ç«¯å®Œå…¨ä¸€è‡´
    [ONNX å…¼å®¹] ä½¿ç”¨æµ®ç‚¹è¿ç®—æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœ
    
    Args:
        x: è¾“å…¥å¼ é‡
        exp2_inv: é‡åŒ–æŒ‡æ•° (scale = 2^(-exp2_inv))
        zp: é›¶ç‚¹
        bitwidth: ä½å®½ (8/16/32)
        symmetric: å¯¹ç§°é‡åŒ– (å½±å“ zp çš„ä½¿ç”¨æ–¹å¼)
        is_unsigned: æ˜¯å¦ä½¿ç”¨æ— ç¬¦å·èŒƒå›´ (UINT)ï¼Œä¸ symmetric ç‹¬ç«‹
                     - False: INT èŒƒå›´ (-128~127, -32768~32767)
                     - True: UINT èŒƒå›´ (0~255, 0~65535)
    """
    # è®¡ç®— scale
    if exp2_inv >= 0:
        scale = 1.0 / (1 << exp2_inv)
    else:
        scale = float(1 << (-exp2_inv))

    # ç¡®å®šé‡åŒ–èŒƒå›´ï¼šç”± is_unsigned å†³å®š INT/UINT
    if bitwidth == 8:
        qmin, qmax = (0, 255) if is_unsigned else (-128, 127)
    elif bitwidth == 16:
        qmin, qmax = (0, 65535) if is_unsigned else (-32768, 32767)
    else:
        qmin, qmax = (0, 4294967295) if is_unsigned else (-2147483648, 2147483647)

    # é‡åŒ–: q = clamp(round(x / scale) + zp, qmin, qmax)
    # æ³¨æ„: torch.round ä½¿ç”¨é“¶è¡Œå®¶èˆå…¥ï¼Œä¸ CUDA çš„ round half up ç•¥æœ‰å·®å¼‚
    # ä½†å®é™…å½±å“æå° (éšæœºæ•°æ®å·®å¼‚ç‡ < 0.001%)
    q = torch.clamp(torch.round(x / scale) + zp, qmin, qmax)

    # åé‡åŒ–: x' = (q - zp) * scale
    x_dequant = (q - zp) * scale

    return x_dequant


def fake_quantize_per_channel(x: torch.Tensor, exp2_invs: list, zp: int = 0,
                              bitwidth: int = 8, symmetric: bool = True,
                              is_unsigned: bool = False) -> torch.Tensor:
    """
    Per-channel ä¼ªé‡åŒ–
    
    [ä¸ CUDA ä¸€è‡´] per-channel é‡åŒ–å‚æ•°ä¸ CUDA quantificationPerChannel ä¸€è‡´
    [ONNX å…¼å®¹] ä½¿ç”¨æµ®ç‚¹è¿ç®—æ¨¡æ‹Ÿé‡åŒ–æ•ˆæœ
    
    Args:
        x: è¾“å…¥å¼ é‡
        exp2_invs: per-channel é‡åŒ–æŒ‡æ•°åˆ—è¡¨
        zp: é›¶ç‚¹
        bitwidth: ä½å®½ (8/16/32)
        symmetric: å¯¹ç§°é‡åŒ–
        is_unsigned: æ˜¯å¦ä½¿ç”¨æ— ç¬¦å·èŒƒå›´ (UINT)
    """
    # ç¡®å®šé‡åŒ–èŒƒå›´ï¼šç”± is_unsigned å†³å®š INT/UINT
    if bitwidth == 8:
        qmin, qmax = (0, 255) if is_unsigned else (-128, 127)
    elif bitwidth == 16:
        qmin, qmax = (0, 65535) if is_unsigned else (-32768, 32767)
    else:
        qmin, qmax = (0, 4294967295) if is_unsigned else (-2147483648, 2147483647)

    device = x.device
    result = torch.zeros_like(x)
    channel_size = len(exp2_invs)

    for c in range(channel_size):
        exp2_inv = exp2_invs[c]
        if exp2_inv >= 0:
            scale = 1.0 / (1 << exp2_inv)
        else:
            scale = float(1 << (-exp2_inv))

        q = torch.clamp(torch.round(x[..., c] / scale) + zp, qmin, qmax)
        result[..., c] = (q - zp) * scale

    return result


# ============================================================
#                   GRUFunction (autograd.Function)
# ============================================================
#
# PyTorch è‡ªå®šä¹‰ç®—å­ï¼Œè¿æ¥ Python å±‚ä¸ C++ CUDA å®ç°ã€‚
# è´Ÿè´£ï¼š
#   1. æƒé‡æ ¼å¼è½¬æ¢ï¼ˆPyTorch â†” Hasteï¼‰
#   2. è°ƒç”¨ C++ forward/backward æ¥å£
#   3. ç®¡ç†æ¢¯åº¦è®¡ç®—å’Œä¸­é—´å˜é‡ä¿å­˜

class GRUFunction(torch.autograd.Function):
    """
    GRU è‡ªå®šä¹‰ autograd Function
    
    èŒè´£ï¼š
        - forward: æƒé‡æ ¼å¼è½¬æ¢ â†’ è°ƒç”¨ gru_ops.forward_interface â†’ è¿”å›è¾“å‡º
        - backward: æ¢¯åº¦æ ¼å¼è½¬æ¢ â†’ è°ƒç”¨ gru_ops.haste_gru_backward â†’ è¿”å›æ¢¯åº¦
    """

    @staticmethod
    def forward(ctx, input, weight_ih, weight_hh, bias_ih, bias_hh, h0, is_training,
                use_quantization=False, quant_params=None):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            weight_ih: [3*H, I] è¾“å…¥æƒé‡ (PyTorch r,z,n æ ¼å¼)
            weight_hh: [3*H, H] å¾ªç¯æƒé‡
            bias_ih, bias_hh: [3*H] åç½®æˆ– None
            h0: [B, H] åˆå§‹çŠ¶æ€æˆ– None
            is_training: è®­ç»ƒæ¨¡å¼æ ‡å¿—
            use_quantization: é‡åŒ–å¼€å…³
            quant_params: é‡åŒ–å‚æ•°
            
        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆçŠ¶æ€
        """
        time_steps, batch_size, input_size = input.shape
        hidden_size = weight_hh.shape[1]

        # ä¿å­˜ç»´åº¦ä¿¡æ¯å’Œ None æ ‡å¿—
        ctx.time_steps, ctx.batch_size = time_steps, batch_size
        ctx.input_size, ctx.hidden_size = input_size, hidden_size
        ctx.bias_ih_is_none = (bias_ih is None)
        ctx.bias_hh_is_none = (bias_hh is None)
        ctx.h0_is_none = (h0 is None)

        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # æƒé‡æ ¼å¼è½¬æ¢(ä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°)
        W, R, bx, br = convert_weights_to_haste_format(
            weight_ih, weight_hh, bias_ih, bias_hh, hidden_size, device
        )

        # åˆå§‹çŠ¶æ€
        h0_tensor = ensure_cuda_float32(h0, device) if h0 is not None else torch.empty(0, device=device,
                                                                                       dtype=torch.float32)

        # é‡åŒ–å‚æ•°
        if use_quantization:
            if quant_params is None:
                raise RuntimeError("use_quantization=True æ—¶å¿…é¡»æä¾› quant_params")
        else:
            quant_params = gru_ops.GRUQuantitativeParameters()

        # è°ƒç”¨ C++ å‰å‘æ¥å£
        output_full, v = gru_ops.forward_interface(
            is_training=is_training,
            is_quant=use_quantization,
            time_steps=time_steps,
            batch_size=batch_size,
            input_size=input_size,
            hidden_size=hidden_size,
            W=W,
            R=R,
            bx=bx,
            br=br,
            x=input,
            h0=h0_tensor,
            quant_params=quant_params
        )

        # åˆ†ç¦»è¾“å‡º: output_full[0] æ˜¯åˆå§‹çŠ¶æ€ï¼Œ[1:] æ˜¯æ—¶é—´æ­¥è¾“å‡º
        output = output_full[1:]
        h_n = output_full[-1:]

        # ä¿å­˜åå‘ä¼ æ’­æ‰€éœ€çš„ä¸­é—´ç»“æœ
        ctx.save_for_backward(W, R, bx, br, input, output_full, v)

        return output, h_n

    @staticmethod
    def backward(ctx, grad_output, grad_h_n):
        """
        åå‘ä¼ æ’­
        
        Args:
            grad_output: [T, B, H] è¾“å‡ºæ¢¯åº¦
            grad_h_n: [1, B, H] æœ€ç»ˆçŠ¶æ€æ¢¯åº¦
            
        Returns:
            å¯¹åº” forward å„å‚æ•°çš„æ¢¯åº¦
        """
        W, R, bx, br, input, h, v = ctx.saved_tensors
        time_steps, batch_size = ctx.time_steps, ctx.batch_size
        input_size, hidden_size = ctx.input_size, ctx.hidden_size

        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨ CUDA ä¸Š
        device = grad_output.device
        tensors = [W, R, bx, br, input, h]
        W, R, bx, br, input, h = [t.to(device) if not t.is_cuda else t for t in tensors]
        if v is not None and not v.is_cuda:
            v = v.to(device)
        if not grad_output.is_cuda:
            grad_output = grad_output.to(device)
        if grad_h_n is not None and not grad_h_n.is_cuda:
            grad_h_n = grad_h_n.to(device)

        # æ„å»ºéšè—çŠ¶æ€æ¢¯åº¦
        # C++ æ¥å£éœ€è¦ [T+1, B, H] æ ¼å¼
        # dh_new[0] æ˜¯åˆå§‹çŠ¶æ€æ¢¯åº¦(ä¿æŒä¸º 0)ï¼Œdh_new[1:] æ˜¯æ—¶é—´æ­¥æ¢¯åº¦
        dh_new = torch.zeros(
            (time_steps + 1, batch_size, hidden_size),
            device=device, dtype=grad_output.dtype
        )
        dh_new[1:] = grad_output

        # ç´¯åŠ æœ€ç»ˆçŠ¶æ€æ¢¯åº¦(output[-1] å’Œ h_n[0] æŒ‡å‘åŒä¸€æ—¶é—´æ­¥)
        if grad_h_n is not None and grad_h_n.numel() > 0:
            dh_new[-1] = dh_new[-1] + grad_h_n[0]

        # è°ƒç”¨ C++ åå‘æ¥å£(ç»‘å®šå±‚ä¼šå¤„ç†æ ¼å¼è½¬æ¢)
        dx, dW, dR, dbx, dbr, dh = gru_ops.haste_gru_backward(
            time_steps=time_steps, batch_size=batch_size,
            input_size=input_size, hidden_size=hidden_size,
            W=W, R=R, bx=bx, br=br, x=input,
            dh_new=dh_new, h=h, v=v
        )

        # æ¢¯åº¦æ ¼å¼è½¬æ¢: Haste (z,r,n) -> PyTorch (r,z,n)
        dW_pytorch = reorder_weights_haste_to_pytorch(dW.t()).contiguous()
        dR_pytorch = reorder_weights_haste_to_pytorch(dR.t()).contiguous()
        dbx_pytorch = reorder_weights_haste_to_pytorch(dbx).contiguous() if not ctx.bias_ih_is_none else None
        dbr_pytorch = reorder_weights_haste_to_pytorch(dbr).contiguous() if not ctx.bias_hh_is_none else None
        grad_h0 = None if ctx.h0_is_none else dh

        # è¿”å›æ¢¯åº¦(å¯¹åº” forward çš„ 9 ä¸ªå‚æ•°)
        return dx, dW_pytorch, dR_pytorch, dbx_pytorch, dbr_pytorch, grad_h0, None, None, None


# ============================================================
#                      QuantGRU æ ¸å¿ƒæ¨¡å—
# ============================================================
#
# QuantGRU æ˜¯æœ¬æ¨¡å—çš„æ ¸å¿ƒç±»ï¼Œæä¾›ï¼š
#   - å…¼å®¹ nn.GRU çš„æ¥å£
#   - INT8/16/32 æ··åˆç²¾åº¦é‡åŒ–æ¨ç†
#   - å¤šç§æ ¡å‡†æ–¹æ³•ï¼ˆMinMax/SQNR/Percentileï¼‰
#   - ONNX å¯¼å‡ºæ”¯æŒï¼ˆfloat/QDQ æ ¼å¼ï¼‰
#
# å†…éƒ¨çŠ¶æ€ç®¡ç†ï¼š
#   - _bitwidth_config: C++ OperatorQuantConfig å¯¹è±¡ï¼ˆä½å®½é…ç½®ï¼‰
#   - _quant_params_dirty: è„æ ‡å¿—ï¼ˆé…ç½®ä¿®æ”¹æˆ–æ ¡å‡†æ•°æ®å˜åŒ–æ—¶ç½® Trueï¼‰
#   - quant_params: é‡åŒ–å‚æ•°ï¼ˆfinalize_calibration åç”Ÿæˆï¼‰

class QuantGRU(nn.Module):
    """
    æ”¯æŒé‡åŒ–çš„ GRU å®ç°ï¼Œå…¼å®¹ nn.GRU æ¥å£
    
    Args:
        input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_size: éšè—çŠ¶æ€ç»´åº¦
        num_layers: å±‚æ•°ï¼ˆä»…æ”¯æŒ 1ï¼‰
        bias: æ˜¯å¦ä½¿ç”¨åç½®ï¼ˆé»˜è®¤ Trueï¼‰
        batch_first: True æ—¶è¾“å…¥ä¸º [B, T, I]ï¼ŒFalse æ—¶ä¸º [T, B, I]ï¼ˆé»˜è®¤ Falseï¼‰
        dropout: æš‚ä¸æ”¯æŒï¼Œå¿…é¡»ä¸º 0
        bidirectional: æ˜¯å¦åŒå‘ï¼ˆé»˜è®¤ Falseï¼‰
        use_quantization: æ˜¯å¦å¯ç”¨é‡åŒ–ï¼ˆé»˜è®¤ Falseï¼‰
    
    Attributes:
        use_quantization (bool): é‡åŒ–å¼€å…³
        calibrating (bool): æ ¡å‡†æ¨¡å¼å¼€å…³ï¼ŒTrue æ—¶ forward ä¼šæ”¶é›†æ ¡å‡†æ•°æ®
        calibration_method (str): æ ¡å‡†æ–¹æ³• 'minmax'|'sqnr'|'percentile'ï¼ˆé»˜è®¤ 'sqnr'ï¼‰
        percentile_value (float): ç™¾åˆ†ä½å€¼ï¼Œä»… 'percentile' æ–¹æ³•ä½¿ç”¨ï¼ˆé»˜è®¤ 99.99ï¼‰
        export_mode (bool): ONNX å¯¼å‡ºæ¨¡å¼ï¼ŒTrue æ—¶ä½¿ç”¨çº¯ PyTorch å®ç°
        export_format (str): å¯¼å‡ºæ ¼å¼ 'float'|'qdq'ï¼ˆé»˜è®¤ 'float'ï¼‰
    
    Example:
        >>> gru = QuantGRU(64, 128, batch_first=True).cuda()
        >>> gru.calibrating = True
        >>> _ = gru(calibration_data)  # æ”¶é›†æ ¡å‡†æ•°æ®
        >>> gru.calibrating = False
        >>> gru.use_quantization = True
        >>> output, h_n = gru(x)  # é‡åŒ–æ¨ç†
    
    Note:
        - ä»…æ”¯æŒå•å±‚ GRUï¼ˆnum_layers=1ï¼‰
        - ä¸æ”¯æŒ dropout
        - é‡åŒ–æ¨ç†éœ€è¦å…ˆæ ¡å‡†ï¼ˆè®¾ç½® calibrating=True å¹¶è¿è¡Œ forwardï¼‰
        - æ”¯æŒ pickle/deepcopyï¼Œä½†æ ¡å‡†æ•°æ®ä¸ä¼šè¢«ä¿å­˜ï¼ˆä½å®½é…ç½®ä¼šä¿ç•™ï¼‰
        - pickle/deepcopy åå¦‚éœ€é‡åŒ–æ¨ç†ï¼Œå¿…é¡»é‡æ–°æ ¡å‡†
    """

    def __init__(
            self,
            input_size: int,
            hidden_size: int,
            num_layers: int = 1,
            bias: bool = True,
            batch_first: bool = False,
            dropout: float = 0.0,
            bidirectional: bool = False,
            use_quantization: bool = False,
    ):
        super(QuantGRU, self).__init__()

        if num_layers != 1:
            raise NotImplementedError("ä»…æ”¯æŒ num_layers=1")
        if dropout > 0:
            raise NotImplementedError("æš‚ä¸æ”¯æŒ dropout")

        # åŸºæœ¬é…ç½®
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.use_quantization = use_quantization
        self.num_directions = 2 if bidirectional else 1

        # ONNX å¯¼å‡ºå¼€å…³ï¼šTrue æ—¶ä½¿ç”¨çº¯ PyTorch å®ç°ï¼Œå¯è¢« ONNX è¿½è¸ª
        self.export_mode = False
        # å¯¼å‡ºæ ¼å¼(é«˜çº§é€‰é¡¹ï¼Œä»…åœ¨ export_mode=True æ—¶æœ‰æ•ˆ)
        # 'float': æµ®ç‚¹(é»˜è®¤ï¼Œä¸ Haste GRU è¡Œä¸ºä¸€è‡´)
        # 'qdq': QDQ ä¼ªé‡åŒ–(æ¨èç”¨äºé‡åŒ–æ¨¡å‹)
        self._export_format = 'float'

        # æƒé‡å‚æ•°(å‘½åä¸ nn.GRU ä¸€è‡´)
        self.weight_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size, input_size))
        self.weight_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
        if bias:
            self.bias_ih_l0 = nn.Parameter(torch.empty(3 * hidden_size))
            self.bias_hh_l0 = nn.Parameter(torch.empty(3 * hidden_size))
        else:
            self.register_parameter('bias_ih_l0', None)
            self.register_parameter('bias_hh_l0', None)

        # åå‘æƒé‡(åŒå‘æ—¶)
        if bidirectional:
            self.weight_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, input_size))
            self.weight_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size, hidden_size))
            if bias:
                self.bias_ih_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
                self.bias_hh_l0_reverse = nn.Parameter(torch.empty(3 * hidden_size))
            else:
                self.register_parameter('bias_ih_l0_reverse', None)
                self.register_parameter('bias_hh_l0_reverse', None)

        self.reset_parameters()

        # é‡åŒ–çŠ¶æ€(å»¶è¿Ÿåˆ›å»º)
        self.quant_ranges = None  # calibrate() æ—¶åˆ›å»º
        self.quant_params = None  # finalize_calibration() æ—¶åˆ›å»º
        if bidirectional:
            self.quant_ranges_reverse = None
            self.quant_params_reverse = None

        # ç»Ÿä¸€è„æ ‡å¿—ï¼šæ ‡è®°é‡åŒ–å‚æ•°æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆæ ¡å‡†æ•°æ®å˜åŒ–æˆ–é…ç½®ä¿®æ”¹éƒ½ä¼šè®¾ç½®ï¼‰
        self._quant_params_dirty = False

        # ä½å®½é…ç½®å¯¹è±¡ï¼ˆç›´æ¥åˆå§‹åŒ–ï¼Œé¿å…å»¶è¿Ÿåˆ›å»ºçš„çº¿ç¨‹å®‰å…¨é—®é¢˜ï¼‰
        self._bitwidth_config = gru_ops.OperatorQuantConfig()  # ä½å®½é…ç½®(ç›´æ¥å­˜å‚¨ C++ å¯¹è±¡)

        self._cublas_initialized = False  # CUDA å»¶è¿Ÿåˆå§‹åŒ–æ ‡å¿—

        # æ ¡å‡†æ–¹æ³•:
        #   - 'minmax': ä½¿ç”¨ min/max èŒƒå›´(å¿«é€Ÿï¼Œæ— ç›´æ–¹å›¾)
        #   - 'sqnr': SQNR ä¼˜åŒ–æœç´¢æœ€ä¼˜ scale(åŸºäºç›´æ–¹å›¾ï¼Œé«˜ç²¾åº¦)
        #   - 'percentile': ç™¾åˆ†ä½è£å‰ª(åŸºäºç›´æ–¹å›¾)
        self.calibration_method = 'sqnr'

        # Percentile é…ç½®(ä»… calibration_method='percentile' æ—¶ä½¿ç”¨)
        self.percentile_value = 99.99

        # ç›´æ–¹å›¾æ”¶é›†å™¨(sqnr/percentile æ–¹æ³•ä½¿ç”¨)
        self.hist_collectors = None
        if bidirectional:
            self.hist_collectors_reverse = None

        # æ ¡å‡†æ¨¡å¼æ ‡å¿—ï¼šå½“ä¸º True æ—¶ï¼Œforward() ä¼šåŒæ—¶æ”¶é›†æ ¡å‡†æ•°æ®
        self.calibrating = False

    def __getstate__(self):
        """
        åºåˆ—åŒ–çŠ¶æ€ï¼ˆç”¨äº pickle/deepcopyï¼‰
        
        å°† C++ æ‰©å±•å¯¹è±¡è½¬æ¢ä¸º Python å­—å…¸ï¼Œä½¿ QuantGRU å¯è¢«åºåˆ—åŒ–ã€‚
        
        Note:
            - ä½å®½é…ç½®ä¼šè¢«ä¿ç•™
            - æ ¡å‡†æ•°æ®ï¼ˆquant_params ç­‰ï¼‰ä¸ä¼šè¢«ä¿å­˜ï¼Œååºåˆ—åŒ–åéœ€é‡æ–°æ ¡å‡†
        """
        state = self.__dict__.copy()
        
        # å°† _bitwidth_config (C++ å¯¹è±¡) è½¬æ¢ä¸º Python å­—å…¸
        if self._bitwidth_config is not None:
            bitwidth_dict = {}
            for json_key, (bw_attr, sym_attr) in _BITWIDTH_FIELD_MAP.items():
                bitwidth_dict[bw_attr] = getattr(self._bitwidth_config, bw_attr)
                bitwidth_dict[sym_attr] = getattr(self._bitwidth_config, sym_attr)
            state['_bitwidth_config'] = bitwidth_dict
        
        # C++ å¯¹è±¡æ— æ³•åºåˆ—åŒ–ï¼Œè®¾ä¸º Noneï¼ˆååºåˆ—åŒ–åéœ€é‡æ–°æ ¡å‡†ï¼‰
        state['quant_ranges'] = None
        state['quant_params'] = None
        state['hist_collectors'] = None
        if self.bidirectional:
            state['quant_ranges_reverse'] = None
            state['quant_params_reverse'] = None
            state['hist_collectors_reverse'] = None
        
        # é‡ç½®è¿è¡Œæ—¶çŠ¶æ€ï¼ˆååºåˆ—åŒ–åéœ€è¦é‡æ–°åˆå§‹åŒ–ï¼‰
        state['_cublas_initialized'] = False
        state['_quant_params_dirty'] = False
        
        return state

    def __setstate__(self, state):
        """
        ååºåˆ—åŒ–çŠ¶æ€
        
        ä» Python å­—å…¸é‡å»º C++ æ‰©å±•å¯¹è±¡ã€‚
        
        Note:
            ååºåˆ—åŒ–åå¦‚éœ€é‡åŒ–æ¨ç†ï¼Œå¿…é¡»é‡æ–°æ ¡å‡†ã€‚
        """
        # æ¢å¤ _bitwidth_config
        bitwidth_dict = state.get('_bitwidth_config')
        if isinstance(bitwidth_dict, dict):
            # ä»å­—å…¸é‡å»º C++ å¯¹è±¡
            config = gru_ops.OperatorQuantConfig()
            for attr, value in bitwidth_dict.items():
                setattr(config, attr, value)
            state['_bitwidth_config'] = config
        elif bitwidth_dict is None:
            # åˆ›å»ºé»˜è®¤é…ç½®
            state['_bitwidth_config'] = gru_ops.OperatorQuantConfig()
        
        self.__dict__.update(state)

    def reset_parameters(self):
        """æƒé‡åˆå§‹åŒ–(ä¸ nn.GRU ç›¸åŒçš„å‡åŒ€åˆ†å¸ƒ)"""
        stdv = 1.0 / (self.hidden_size ** 0.5)
        for param in self.parameters():
            nn.init.uniform_(param, -stdv, stdv)

    # -------------------- å†…éƒ¨æ–¹æ³• --------------------

    def _ensure_cublas_initialized(self):
        """å»¶è¿Ÿåˆå§‹åŒ– cublas handle"""
        if not self._cublas_initialized:
            gru_ops.init_gru_cublas()
            self._cublas_initialized = True

    def _use_histogram_collection(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ç›´æ–¹å›¾æ”¶é›†(sqnr/percentile éƒ½éœ€è¦)"""
        return self.calibration_method in ('sqnr', 'percentile')

    def _ensure_calibration_collectors(self, hidden_size: int, reverse: bool = False):
        """
        ç¡®ä¿æ ¡å‡†æ”¶é›†å™¨å·²åˆå§‹åŒ–(ç»Ÿä¸€æ¥å£)
        
        æ ¹æ® calibration_method è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„æ”¶é›†å™¨ç±»å‹
        """
        use_histogram = self._use_histogram_collection()

        if reverse:
            if use_histogram:
                if self.hist_collectors_reverse is None:
                    self.hist_collectors_reverse = gru_ops.GRUHistogramCollectors(hidden_size, num_bins=2048)
            else:
                if self.quant_ranges_reverse is None:
                    self.quant_ranges_reverse = gru_ops.GRUQuantizationRanges(hidden_size)
        else:
            if use_histogram:
                if self.hist_collectors is None:
                    self.hist_collectors = gru_ops.GRUHistogramCollectors(hidden_size, num_bins=2048)
            else:
                if self.quant_ranges is None:
                    self.quant_ranges = gru_ops.GRUQuantizationRanges(hidden_size)

    def _get_calibration_args(self, reverse: bool = False) -> tuple:
        """
        è·å–æ ¡å‡†å‚æ•°(ç»Ÿä¸€æ¥å£)
        
        Returns:
            (hist_collectors, quant_ranges) - æ ¹æ®æ ¡å‡†æ–¹æ³•è¿”å›æ­£ç¡®çš„æ”¶é›†å™¨
        """
        use_histogram = self._use_histogram_collection()
        if reverse:
            return (
                self.hist_collectors_reverse if use_histogram else None,
                self.quant_ranges_reverse if not use_histogram else None
            )
        else:
            return (
                self.hist_collectors if use_histogram else None,
                self.quant_ranges if not use_histogram else None
            )

    def _parse_initial_state(
            self,
            hx: Optional[torch.Tensor],
            batch_size: int,
            device: torch.device = None,
            to_cuda: bool = False
    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:
        """
        è§£æåˆå§‹éšè—çŠ¶æ€(ç»Ÿä¸€æ¥å£)
        
        Args:
            hx: åˆå§‹éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ [num_directions, B, H] æˆ– None
            batch_size: æ‰¹æ¬¡å¤§å°
            device: ç›®æ ‡è®¾å¤‡(to_cuda=True æ—¶ä½¿ç”¨)
            to_cuda: æ˜¯å¦è½¬æ¢ä¸º CUDA float32
            
        Returns:
            (h0_forward, h0_reverse): å‰å‘å’Œåå‘åˆå§‹çŠ¶æ€
        """
        h0_forward, h0_reverse = None, None
        if hx is not None:
            expected_layers = self.num_layers * self.num_directions
            expected_shape = (expected_layers, batch_size, self.hidden_size)
            if hx.shape != expected_shape:
                raise ValueError(f"hx å½¢çŠ¶åº”ä¸º {expected_shape}ï¼Œå®é™… {hx.shape}")
            h0_forward = ensure_cuda_float32(hx[0], device) if to_cuda else hx[0]
            if self.bidirectional:
                h0_reverse = ensure_cuda_float32(hx[1], device) if to_cuda else hx[1]
        return h0_forward, h0_reverse

    def _combine_bidirectional_outputs(
            self,
            output_forward: torch.Tensor,
            h_n_forward: torch.Tensor,
            output_reverse: Optional[torch.Tensor] = None,
            h_n_reverse: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åˆå¹¶åŒå‘ GRU è¾“å‡º(ç»Ÿä¸€æ¥å£)
        
        Args:
            output_forward: å‰å‘è¾“å‡º [T, B, H]
            h_n_forward: å‰å‘æœ€ç»ˆçŠ¶æ€ [1, B, H]
            output_reverse: åå‘è¾“å‡º [T, B, H](å·²ç¿»è½¬æˆ–æœªç¿»è½¬å‡å¯)
            h_n_reverse: åå‘æœ€ç»ˆçŠ¶æ€ [1, B, H]
            
        Returns:
            (output, h_n): åˆå¹¶åçš„è¾“å‡ºå’ŒçŠ¶æ€
        """
        if self.bidirectional and output_reverse is not None:
            # æ‹¼æ¥è¾“å‡º: [T, B, H] + [T, B, H] -> [T, B, 2H]
            output = torch.cat([output_forward, output_reverse], dim=-1)
            # æ‹¼æ¥éšè—çŠ¶æ€: [1, B, H] + [1, B, H] -> [2, B, H]
            h_n = torch.cat([h_n_forward, h_n_reverse], dim=0)
        else:
            output = output_forward
            h_n = h_n_forward
        return output, h_n

    # -------------------- å…¬å¼€æ¥å£ --------------------

    def load_bitwidth_config(self, config_file: str, verbose: bool = False):
        """
        ä» JSON æ–‡ä»¶åŠ è½½ä½å®½é…ç½®ï¼ˆ2 å±‚è®¾è®¡ï¼šJSON â†’ C++ å¯¹è±¡ï¼‰
        
        Args:
            config_file: JSON é…ç½®æ–‡ä»¶è·¯å¾„
            verbose: æ˜¯å¦æ‰“å°é…ç½®ä¿¡æ¯
        """
        import warnings

        # è§£æ JSON æ–‡ä»¶
        with open(config_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # è¯»å– GRU_config èŠ‚ç‚¹ä¸‹çš„é…ç½®
        gru_config = data.get('GRU_config', {})

        # è¯»å–å…¨å±€é…ç½®
        default_config = gru_config.get('default_config', {})
        if 'disable_quantization' in default_config:
            self.use_quantization = not default_config['disable_quantization']

        # ç›´æ¥å°†é…ç½®å†™å…¥ C++ å¯¹è±¡
        op_config = gru_config.get('operator_config', {})

        # æ£€æŸ¥ JSON ä¸­ç¼ºå¤±çš„å­—æ®µå¹¶å‘å‡ºè­¦å‘Š
        missing_fields = []
        for json_key, (bw_attr, sym_attr) in _BITWIDTH_FIELD_MAP.items():
            if json_key in op_config:
                op_cfg = op_config[json_key]
                setattr(self._bitwidth_config, bw_attr, op_cfg.get('bitwidth', 8))
                setattr(self._bitwidth_config, sym_attr, op_cfg.get('is_symmetric', True))
            else:
                missing_fields.append(json_key)

        if missing_fields:
            warnings.warn(
                f"JSON é…ç½®æ–‡ä»¶ '{config_file}' ç¼ºå°‘ä»¥ä¸‹å­—æ®µï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ (8bit, å¯¹ç§°):\n"
                f"  {missing_fields}",
                UserWarning
            )

        # æ ‡è®°é‡åŒ–å‚æ•°éœ€è¦æ›´æ–°ï¼ˆforward æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨ finalize_calibrationï¼‰
        self._quant_params_dirty = True

        if verbose:
            print_bitwidth_config(self._bitwidth_config, config_file)
            print(f"  [å…¨å±€]  use_quantization: {self.use_quantization}")

    def set_all_bitwidth(self, bitwidth: int = 8, is_symmetric: bool = True, verbose: bool = False):
        """
        è®¾ç½®æ‰€æœ‰ç®—å­ç»Ÿä¸€çš„ä½å®½å’Œå¯¹ç§°é‡åŒ–é…ç½®ï¼ˆ2 å±‚è®¾è®¡ï¼šç›´æ¥æ“ä½œ C++ å¯¹è±¡ï¼‰
        
        Args:
            bitwidth: ä½å®½ (8/16/32)
            is_symmetric: æ˜¯å¦å¯¹ç§°é‡åŒ–(ä»…å¯¹æ¿€æ´»å€¼ç”Ÿæ•ˆï¼Œæƒé‡/åç½®å§‹ç»ˆå¯¹ç§°)
            verbose: æ˜¯å¦æ‰“å°ä¿¡æ¯
        """
        if bitwidth not in (8, 16, 32):
            raise ValueError(f"bitwidth must be 8, 16 or 32, got {bitwidth}")

        # è®¾ç½®æ‰€æœ‰ä½å®½å±æ€§ï¼ˆä½¿ç”¨æ¨¡å—çº§å¸¸é‡ï¼‰
        for attr in _VALID_BITWIDTH_ATTRS:
            setattr(self._bitwidth_config, attr, bitwidth)

        # æƒé‡/åç½®å§‹ç»ˆä½¿ç”¨å¯¹ç§°é‡åŒ–ï¼ˆä½¿ç”¨æ¨¡å—çº§å¸¸é‡ï¼‰
        for attr in _WEIGHT_SYMMETRIC_ATTRS:
            setattr(self._bitwidth_config, attr, True)

        # æ¿€æ´»å€¼å¯¹ç§°é‡åŒ–é…ç½®ç”±å‚æ•°æ§åˆ¶ï¼ˆä½¿ç”¨æ¨¡å—çº§å¸¸é‡ï¼‰
        for attr in _ACTIVATION_SYMMETRIC_ATTRS:
            setattr(self._bitwidth_config, attr, is_symmetric)

        # æ ‡è®°é‡åŒ–å‚æ•°éœ€è¦æ›´æ–°ï¼ˆforward æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨ finalize_calibrationï¼‰
        self._quant_params_dirty = True

        if verbose:
            sym_str = "å¯¹ç§°" if is_symmetric else "éå¯¹ç§°"
            print(f"\n[QuantGRU] è®¾ç½®æ‰€æœ‰ç®—å­: {bitwidth}bit, æ¿€æ´»å€¼{sym_str}é‡åŒ–, æƒé‡/åç½®å¯¹ç§°é‡åŒ–")

    def is_calibrated(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ ¡å‡†"""
        if self.bidirectional:
            return self.quant_params is not None and self.quant_params_reverse is not None
        return self.quant_params is not None

    def finalize_calibration(self, verbose: bool = False):
        """
        å®Œæˆæ ¡å‡†ï¼Œè®¡ç®—é‡åŒ–å‚æ•°å¹¶åˆå§‹åŒ– LUT
        
        Args:
            verbose: æ˜¯å¦æ‰“å°æ ¡å‡†ä¿¡æ¯
            
        Raises:
            RuntimeError: æœªæ”¶é›†æ ¡å‡†æ•°æ®
        """
        use_histogram = self._use_histogram_collection()
        use_percentile = (self.calibration_method == 'percentile')

        # æ£€æŸ¥æ ¡å‡†æ•°æ®
        if use_histogram:
            if self.hist_collectors is None or not self.hist_collectors.is_valid():
                raise RuntimeError("æœªæ”¶é›†æ ¡å‡†æ•°æ®ï¼Œè¯·å…ˆè®¾ç½® calibrating=True å¹¶è°ƒç”¨ forward()")
        else:
            if self.quant_ranges is None:
                raise RuntimeError("æœªæ”¶é›†æ ¡å‡†æ•°æ®ï¼Œè¯·å…ˆè®¾ç½® calibrating=True å¹¶è°ƒç”¨ forward()")

        bitwidth_config = self._bitwidth_config

        if verbose:
            method_name = {
                'minmax': 'MINMAX',
                'sqnr': 'SQNR',
                'percentile': f'PERCENTILE ({self.percentile_value}%)'
            }.get(self.calibration_method, self.calibration_method.upper())
            print(f"\n[QuantGRU] æ ¡å‡†æ–¹æ³•: {method_name}")

        # å‰å‘æ–¹å‘
        if use_histogram:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters_from_histograms(
                hist_collectors=self.hist_collectors,
                bitwidth_config=bitwidth_config,
                verbose=verbose,
                use_percentile=use_percentile,
                percentile_value=self.percentile_value)
        else:
            self.quant_params = gru_ops.calculate_gru_quantitative_parameters(
                quant_ranges=self.quant_ranges, bitwidth_config=bitwidth_config)

        # åå‘æ–¹å‘(åŒå‘æ—¶)
        if self.bidirectional:
            if use_histogram:
                if self.hist_collectors_reverse is None or not self.hist_collectors_reverse.is_valid():
                    raise RuntimeError("åŒå‘ GRU åå‘ç›´æ–¹å›¾æ•°æ®å¼‚å¸¸")
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters_from_histograms(
                    hist_collectors=self.hist_collectors_reverse,
                    bitwidth_config=bitwidth_config,
                    verbose=verbose,
                    use_percentile=use_percentile,
                    percentile_value=self.percentile_value)
            else:
                if self.quant_ranges_reverse is None:
                    raise RuntimeError("åŒå‘ GRU åå‘æ ¡å‡†æ•°æ®å¼‚å¸¸")
                self.quant_params_reverse = gru_ops.calculate_gru_quantitative_parameters(
                    quant_ranges=self.quant_ranges_reverse, bitwidth_config=bitwidth_config)

        # é‡åŒ–å‚æ•°å·²æ›´æ–°ï¼Œæ¸…é™¤è„æ ‡å¿—
        self._quant_params_dirty = False

    def reset_calibration(self):
        """é‡ç½®æ ¡å‡†çŠ¶æ€ï¼Œæ¸…é™¤æ‰€æœ‰ç´¯ç§¯çš„èŒƒå›´å’Œå‚æ•°"""
        self.quant_ranges = None
        self.quant_params = None
        self.hist_collectors = None
        # é‡ç½®æ ¡å‡†åï¼Œè„æ ‡å¿—æ¸…é™¤ï¼ˆä¸‹æ¬¡æ ¡å‡†ä¼šé‡æ–°åº”ç”¨é…ç½®ï¼‰
        self._quant_params_dirty = False
        if self.bidirectional:
            self.quant_ranges_reverse = None
            self.quant_params_reverse = None
            self.hist_collectors_reverse = None

    # -------------------- ONNX å¯¼å‡ºæ¨¡å¼ï¼šçº¯ PyTorch å®ç° --------------------

    def _get_bitwidth(self, op_name: str) -> int:
        """
        è·å–æŒ‡å®šæ“ä½œçš„ä½å®½
        
        Args:
            op_name: æ“ä½œåç§°ï¼ˆå¦‚ 'x', 'h', 'Wx' ç­‰ï¼‰
            
        Returns:
            ä½å®½å€¼ï¼ˆ8/16/32ï¼‰ï¼Œæ— æ•ˆæ“ä½œåè¿”å›é»˜è®¤å€¼ 8 å¹¶å‘å‡ºè­¦å‘Š
        """
        attr_name = f'{op_name}_'

        # éªŒè¯å±æ€§åæ˜¯å¦æœ‰æ•ˆ
        if attr_name not in _VALID_BITWIDTH_ATTRS:
            import warnings
            warnings.warn(
                f"æœªçŸ¥çš„ä½å®½å±æ€§å: '{attr_name}'ï¼Œå°†è¿”å›é»˜è®¤å€¼ 8ã€‚"
                f"æœ‰æ•ˆå±æ€§: {sorted(_VALID_BITWIDTH_ATTRS)}",
                UserWarning
            )
            return 8

        return getattr(self._bitwidth_config, attr_name, 8)

    def _get_symmetric(self, op_name: str) -> bool:
        """
        è·å–æŒ‡å®šæ“ä½œæ˜¯å¦ä½¿ç”¨å¯¹ç§°é‡åŒ–
        
        Args:
            op_name: æ“ä½œåç§°ï¼ˆå¦‚ 'x', 'h', 'Wx' ç­‰ï¼‰
            
        Returns:
            æ˜¯å¦å¯¹ç§°é‡åŒ–ï¼Œæ— æ•ˆæ“ä½œåè¿”å›é»˜è®¤å€¼ True å¹¶å‘å‡ºè­¦å‘Š
        """
        attr_name = f'{op_name}_symmetric_'

        # éªŒè¯å±æ€§åæ˜¯å¦æœ‰æ•ˆ
        if attr_name not in _VALID_SYMMETRIC_ATTRS:
            import warnings
            warnings.warn(
                f"æœªçŸ¥çš„å¯¹ç§°é‡åŒ–å±æ€§å: '{attr_name}'ï¼Œå°†è¿”å›é»˜è®¤å€¼ Trueã€‚"
                f"æœ‰æ•ˆå±æ€§: {sorted(_VALID_SYMMETRIC_ATTRS)}",
                UserWarning
            )
            return True

        return getattr(self._bitwidth_config, attr_name, True)

    @property
    def export_format(self) -> str:
        """
        è·å–å¯¼å‡ºæ ¼å¼(é«˜çº§é€‰é¡¹ï¼Œä»…åœ¨ export_mode=True æ—¶æœ‰æ•ˆ)
        
        Returns:
            'float': æµ®ç‚¹æ ¼å¼(é»˜è®¤ï¼Œä¸ Haste GRU è¡Œä¸ºä¸€è‡´)
            'qdq': QDQ ä¼ªé‡åŒ–æ ¼å¼(æ¨èç”¨äºé‡åŒ–æ¨¡å‹ ONNX å¯¼å‡º)
        """
        return self._export_format

    @export_format.setter
    def export_format(self, mode: str):
        """
        è®¾ç½®å¯¼å‡ºæ ¼å¼(é«˜çº§ç”¨æ³•ï¼Œå¤§å¤šæ•°ç”¨æˆ·ä¸éœ€è¦ä¿®æ”¹)
        
        Args:
            mode: 'qdq' | 'float'
        """
        valid_modes = ('qdq', 'float')
        if mode not in valid_modes:
            raise ValueError(f"Invalid export_format: '{mode}'. Use one of {valid_modes}")
        self._export_format = mode

    def _forward_python_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
            quant_params
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        çº¯ PyTorch å®ç°çš„å•å‘ GRU å‰å‘ä¼ æ’­(å¯è¢« ONNX è¿½è¸ª)

        GRU å…¬å¼(Haste æ ¼å¼ï¼Œé—¨é¡ºåºä¸º z, r, g)ï¼š
            z = sigmoid(W_z @ x + R_z @ h + bx_z + br_z)  # update gate
            r = sigmoid(W_r @ x + R_r @ h + bx_r + br_r)  # reset gate
            g = tanh(W_g @ x + r * (R_g @ h + br_g) + bx_g)  # candidate gate
            h' = z * h + (1 - z) * g

        é‡åŒ–æ¨¡å¼ä¸‹æ ¹æ® ONNX å¯¼å‡ºæ¨¡å¼é€‰æ‹©å®ç°ï¼š
            - 'qdq': QDQ æ ¼å¼ï¼Œä½¿ç”¨æ ‡å‡†ç®—å­ + ä¼ªé‡åŒ–
            - 'float': æ ‡å‡†æµ®ç‚¹è®¡ç®—(Haste æ ¼å¼)

        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            h0: [B, H] åˆå§‹éšè—çŠ¶æ€ æˆ– None
            weight_ih: [3*H, I] è¾“å…¥æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            weight_hh: [3*H, H] å¾ªç¯æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            bias_ih: [3*H] è¾“å…¥åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            bias_hh: [3*H] å¾ªç¯åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è‡ªåŠ¨è½¬æ¢)
            quant_params: é‡åŒ–å‚æ•°(æ¥è‡ª finalize_calibration)

        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆéšè—çŠ¶æ€
        """
        # æ ¹æ® export_format é€‰æ‹©å®ç°
        if self._export_format == 'float':
            # æµ®ç‚¹æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æµ®ç‚¹å®ç°
            return self._forward_python_float_single_direction(
                input, h0, weight_ih, weight_hh, bias_ih, bias_hh
            )

        # qdq éœ€è¦é‡åŒ–å‚æ•°
        if quant_params is None:
            raise RuntimeError(
                f"export_format='{self._export_format}' éœ€è¦é‡åŒ–å‚æ•°ï¼Œ"
                f"è¯·å…ˆè®¾ç½® calibrating=True å¹¶è°ƒç”¨ forward()"
            )

        if self._export_format == 'qdq':
            return self._forward_onnx_qdq_single_direction(
                input, h0, weight_ih, weight_hh, bias_ih, bias_hh, quant_params
            )

        # ç†è®ºä¸Šä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ(setter å·²é™åˆ¶å€¼)ï¼Œä½†ä¸ºäº†å¥å£®æ€§æŠ›å‡ºå¼‚å¸¸
        raise ValueError(f"æœªçŸ¥çš„ export_format: '{self._export_format}'")

    def _forward_python_float_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        æµ®ç‚¹å®ç°çš„å•å‘ GRU å‰å‘ä¼ æ’­(Haste æ ¼å¼)
        
        ä¸ HasteGRU CUDA æµ®ç‚¹æ¨ç†è¡Œä¸ºä¸€è‡´
        é—¨æ§é¡ºåºï¼šHaste æ ¼å¼ (z, r, g)
        
        å…¬å¼(ä¸ gru_forward_gpu.cu ä¸€è‡´)ï¼š
            z = sigmoid(Wx_z + Rh_z + bx_z + br_z)
            r = sigmoid(Wx_r + Rh_r + bx_r + br_r)
            g = tanh(Wx_g + r * (Rh_g + br_g) + bx_g)
            h_new = z * h_old + (1 - z) * g
        
        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            h0: [B, H] åˆå§‹éšè—çŠ¶æ€ æˆ– None
            weight_ih: [3*H, I] è¾“å…¥æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            weight_hh: [3*H, H] å¾ªç¯æƒé‡ (PyTorch r,z,n æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            bias_ih: [3*H] è¾“å…¥åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            bias_hh: [3*H] å¾ªç¯åç½® æˆ– None (PyTorch æ ¼å¼ï¼Œå†…éƒ¨è½¬æ¢)
            
        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆéšè—çŠ¶æ€
        """
        T, B, I = input.shape
        H = self.hidden_size
        device = input.device
        dtype = input.dtype

        # åˆå§‹åŒ–éšè—çŠ¶æ€
        if h0 is None:
            h = torch.zeros(B, H, device=device, dtype=dtype)
        else:
            h = h0

        # æƒé‡æ ¼å¼è½¬æ¢ï¼šPyTorch (r,z,n) -> Haste (z,r,g)
        W = reorder_weights_pytorch_to_haste(weight_ih)  # [3*H, I]
        R = reorder_weights_pytorch_to_haste(weight_hh)  # [3*H, H]

        # å¤„ç†åç½®å¹¶è½¬æ¢æ ¼å¼
        if bias_ih is None:
            bx = torch.zeros(3 * H, device=device, dtype=dtype)
        else:
            bx = reorder_weights_pytorch_to_haste(bias_ih)
        if bias_hh is None:
            br = torch.zeros(3 * H, device=device, dtype=dtype)
        else:
            br = reorder_weights_pytorch_to_haste(bias_hh)

        # ========== å¾ªç¯å¤–ä¸€æ¬¡æ€§è®¡ç®— Wx GEMM(ä¸ CUDA ä¸€è‡´)==========
        # input: [T, B, I] -> x_flat: [T*B, I]
        # W: [3*H, I] -> W.t(): [I, 3*H]
        # Wx_all: [T*B, 3*H] -> reshape: [T, B, 3*H]
        x_flat = input.reshape(T * B, I)
        Wx_all = torch.mm(x_flat, W.t())  # [T*B, 3*H]
        Wx_all = Wx_all.reshape(T, B, 3 * H)  # [T, B, 3*H]

        # é¢„åˆ†å‰²åç½®(å¾ªç¯å¤–å®Œæˆ)
        bx_z, bx_r, bx_g = bx.chunk(3)
        br_z, br_r, br_g = br.chunk(3)

        outputs = []

        for t in range(T):
            # è·å–å½“å‰æ—¶é—´æ­¥çš„ Wx(å·²åœ¨å¾ªç¯å¤–è®¡ç®—å¥½)
            Wx = Wx_all[t]  # [B, 3*H]

            # Rh = h @ R.T, shape [B, 3H](ä¾èµ–ä¸Šä¸€æ­¥çš„ hï¼Œå¿…é¡»åœ¨å¾ªç¯å†…)
            Rh = torch.mm(h, R.t())

            # åˆ†å‰²é—¨æ§(Haste æ ¼å¼ï¼šz, r, g)
            Wx_z, Wx_r, Wx_g = Wx.chunk(3, dim=1)
            Rh_z, Rh_r, Rh_g = Rh.chunk(3, dim=1)

            # Update gate (z)
            z = torch.sigmoid(Wx_z + Rh_z + bx_z + br_z)

            # Reset gate (r)
            r = torch.sigmoid(Wx_r + Rh_r + bx_r + br_r)

            # Candidate gate (g): r åªä¹˜ä»¥ (Rh_g + br_g)
            Rh_add_br_g = Rh_g + br_g
            g = torch.tanh(Wx_g + r * Rh_add_br_g + bx_g)

            # æ–°éšè—çŠ¶æ€: h_new = z * h_old + (1 - z) * g
            h = z * h + (1 - z) * g

            outputs.append(h)

        # å †å è¾“å‡º: [T, B, H]
        output = torch.stack(outputs, dim=0)
        h_n = h.unsqueeze(0)  # [1, B, H]

        return output, h_n

    # -------------------- ONNX å¯¼å‡ºç‰ˆæœ¬(QDQ æ ¼å¼)--------------------

    def _forward_onnx_qdq_single_direction(
            self,
            input: torch.Tensor,
            h0: Optional[torch.Tensor],
            weight_ih: torch.Tensor,
            weight_hh: torch.Tensor,
            bias_ih: Optional[torch.Tensor],
            bias_hh: Optional[torch.Tensor],
            quant_params
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ç”¨äº ONNX å¯¼å‡ºçš„ QDQ æ ¼å¼å‰å‘ä¼ æ’­
        
        ä½¿ç”¨ä¼ªé‡åŒ–(Fake Quantize)åœ¨å…³é”®ç‚¹æ’å…¥ Q/DQ æ“ä½œï¼Œ
        æ¨ç†å¼•æ“ä¼šè¯†åˆ« QDQ æ¨¡å¼å¹¶è‡ªåŠ¨ä¼˜åŒ–ä¸ºé‡åŒ–ç®—å­ã€‚
        
        è®¾è®¡åŸåˆ™ï¼š
        ==========
        [ä¸ CUDA ä¸€è‡´]
          - é‡åŒ–å‚æ•°(scale/zp)å®Œå…¨ä¸€è‡´
          - è®¡ç®—å›¾ç»“æ„ä¸€è‡´(é—¨é¡ºåºã€è®¡ç®—é¡ºåº)
          - æƒé‡/åç½®çš„ per-channel é‡åŒ–å‚æ•°ä¸€è‡´
          
        [ONNX å…¼å®¹ - ä¸ CUDA å®ç°ä¸åŒ]
          - GEMM: ä½¿ç”¨æ ‡å‡† torch.mm(æ¨ç†å¼•æ“ä¼šç”¨ MatMulInteger)
          - sigmoid/tanh: ä½¿ç”¨æ ‡å‡† torch.sigmoid/tanh(æ¨ç†å¼•æ“ä¼šä¼˜åŒ–)
          - rescale: é€šè¿‡ QDQ å®ç°(ä¸ç”¨æ˜¾å¼ rshift_round)
        
        Args:
            input: [T, B, I] è¾“å…¥åºåˆ—
            h0: [B, H] åˆå§‹éšè—çŠ¶æ€ æˆ– None
            weight_ih: [3*H, I] è¾“å…¥æƒé‡
            weight_hh: [3*H, H] å¾ªç¯æƒé‡
            bias_ih: [3*H] è¾“å…¥åç½® æˆ– None
            bias_hh: [3*H] å¾ªç¯åç½® æˆ– None
            quant_params: é‡åŒ–å‚æ•°
            
        Returns:
            output: [T, B, H] è¾“å‡ºåºåˆ—
            h_n: [1, B, H] æœ€ç»ˆéšè—çŠ¶æ€
        """
        T, B, I = input.shape
        H = self.hidden_size
        device = input.device
        dtype = input.dtype

        # ========== é‡åŒ–å‚æ•°æå– ==========
        # [ä¸ CUDA ä¸€è‡´] ä½¿ç”¨ç›¸åŒçš„é‡åŒ–å‚æ•°
        exp2_x = quant_params.exp2_inv_x_
        zp_x = quant_params.zp_x_
        exp2_h = quant_params.exp2_inv_h_
        zp_h = quant_params.zp_h_
        exp2_Wx = quant_params.exp2_inv_Wx_
        zp_Wx = quant_params.zp_Wx_
        exp2_Rh = quant_params.exp2_inv_Rh_
        zp_Rh = quant_params.zp_Rh_

        # æ¿€æ´»å‡½æ•°é‡åŒ–å‚æ•°
        exp2_z_pre = quant_params.exp2_inv_z_pre_
        zp_z_pre = quant_params.zp_z_pre_
        exp2_z_out = quant_params.exp2_inv_z_out_
        zp_z_out = quant_params.zp_z_out_

        exp2_r_pre = quant_params.exp2_inv_r_pre_
        zp_r_pre = quant_params.zp_r_pre_
        exp2_r_out = quant_params.exp2_inv_r_out_
        zp_r_out = quant_params.zp_r_out_

        exp2_g_pre = quant_params.exp2_inv_g_pre_
        zp_g_pre = quant_params.zp_g_pre_
        exp2_g_out = quant_params.exp2_inv_g_out_
        zp_g_out = quant_params.zp_g_out_

        # per-channel é‡åŒ–å‚æ•°
        exp2_W = list(quant_params.exp2_inv_W_)
        exp2_R = list(quant_params.exp2_inv_R_)
        exp2_bx = list(quant_params.exp2_inv_bx_)
        exp2_br = list(quant_params.exp2_inv_br_)

        # ========== æƒé‡é‡æ’åº ==========
        # [ä¸ CUDA ä¸€è‡´] PyTorch æ ¼å¼ (r, z, n) -> Haste æ ¼å¼ (z, r, n)
        W_reordered = reorder_weights_pytorch_to_haste(weight_ih)  # [3*H, I]
        R_reordered = reorder_weights_pytorch_to_haste(weight_hh)  # [3*H, H]

        if bias_ih is not None:
            bx_reordered = reorder_weights_pytorch_to_haste(bias_ih)  # [3*H]
        else:
            bx_reordered = torch.zeros(3 * H, device=device, dtype=dtype)

        if bias_hh is not None:
            br_reordered = reorder_weights_pytorch_to_haste(bias_hh)  # [3*H]
        else:
            br_reordered = torch.zeros(3 * H, device=device, dtype=dtype)

        # ========== æƒé‡ä¼ªé‡åŒ– ==========
        # [ä¸ CUDA ä¸€è‡´] per-channel é‡åŒ–
        # [ONNX å…¼å®¹] ä½¿ç”¨ fake_quantize ä¿æŒæµ®ç‚¹æ ¼å¼
        W_q = fake_quantize_per_channel(W_reordered.t(), exp2_W, zp=0,
                                        bitwidth=self._get_bitwidth('W'),
                                        symmetric=self._get_symmetric('W')).t()
        R_q = fake_quantize_per_channel(R_reordered.t(), exp2_R, zp=0,
                                        bitwidth=self._get_bitwidth('R'),
                                        symmetric=self._get_symmetric('R')).t()
        # åç½®ä½¿ç”¨é…ç½®çš„ä½å®½(æ³¨æ„ï¼šåç½®å§‹ç»ˆä½¿ç”¨å¯¹ç§°é‡åŒ–)
        bx_q = fake_quantize_per_channel(bx_reordered.unsqueeze(0), exp2_bx, zp=0,
                                         bitwidth=self._get_bitwidth('bx'),
                                         symmetric=self._get_symmetric('bx')).squeeze(0)
        br_q = fake_quantize_per_channel(br_reordered.unsqueeze(0), exp2_br, zp=0,
                                         bitwidth=self._get_bitwidth('br'),
                                         symmetric=self._get_symmetric('br')).squeeze(0)

        # åˆ†å‰²åç½®(Haste æ ¼å¼ï¼šz, r, g)
        bx_z, bx_r, bx_g = bx_q.chunk(3)  # å„ [H]
        br_z, br_r, br_g = br_q.chunk(3)  # å„ [H]

        # ========== åˆå§‹åŒ–éšè—çŠ¶æ€ ==========
        if h0 is None:
            h = torch.zeros(B, H, device=device, dtype=dtype)
        else:
            h = h0

        # [ä¸ CUDA ä¸€è‡´] é‡åŒ–åˆå§‹çŠ¶æ€
        h = fake_quantize(h, exp2_h, zp_h, bitwidth=self._get_bitwidth('h'),
                          symmetric=self._get_symmetric('h'))

        # ========== è¾“å…¥ä¼ªé‡åŒ– ==========
        # [ä¸ CUDA ä¸€è‡´] æ‰€æœ‰æ—¶é—´æ­¥ä¸€èµ·é‡åŒ–
        x_q = fake_quantize(input, exp2_x, zp_x, bitwidth=self._get_bitwidth('x'),
                            symmetric=self._get_symmetric('x'))

        # ========== Wx GEMM(å¾ªç¯å¤–ä¸€æ¬¡æ€§è®¡ç®—)==========
        # [ä¸ CUDA ä¸€è‡´] è®¡ç®—é¡ºåºä¸€è‡´
        # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† matmulï¼Œæ¨ç†å¼•æ“ä¼šæ›¿æ¢ä¸º MatMulInteger
        # x_q: [T, B, I], W_q: [3*H, I] -> Wx: [T, B, 3*H]
        Wx_all = torch.matmul(x_q, W_q.t())  # [T, B, 3*H]

        # [ä¸ CUDA ä¸€è‡´] GEMM è¾“å‡ºé‡åŒ–
        Wx_all = fake_quantize(Wx_all, exp2_Wx, zp_Wx, bitwidth=self._get_bitwidth('Wx'),
                               symmetric=self._get_symmetric('Wx'))

        # é¢„åˆ†é…è¾“å‡ºå¼ é‡(ONNX å‹å¥½ï¼Œé¿å…åŠ¨æ€åˆ—è¡¨)
        outputs = torch.zeros(T, B, H, device=device, dtype=dtype)

        for t in range(T):
            Wx = Wx_all[t]  # [B, 3*H]

            # ========== Rh GEMM ==========
            # [ä¸ CUDA ä¸€è‡´] æ¯ä¸ªæ—¶é—´æ­¥è®¡ç®— Rh
            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† matmul
            Rh = torch.mm(h, R_q.t())  # [B, 3*H]

            # [ä¸ CUDA ä¸€è‡´] GEMM è¾“å‡ºé‡åŒ–
            Rh = fake_quantize(Rh, exp2_Rh, zp_Rh, bitwidth=self._get_bitwidth('Rh'),
                               symmetric=self._get_symmetric('Rh'))

            # ========== åˆ†å‰²é—¨æ§ ==========
            # [ä¸ CUDA ä¸€è‡´] Haste æ ¼å¼ (z, r, g)
            Wx_z, Wx_r, Wx_g = Wx.chunk(3, dim=1)  # å„ [B, H]
            Rh_z, Rh_r, Rh_g = Rh.chunk(3, dim=1)  # å„ [B, H]

            # ========== z é—¨(Update Gate)==========
            # [ä¸ CUDA ä¸€è‡´] z = sigmoid(Wx_z + Rh_z + bx_z + br_z)
            z_pre = Wx_z + Rh_z + bx_z.unsqueeze(0) + br_z.unsqueeze(0)

            # [ä¸ CUDA ä¸€è‡´] æ¿€æ´»å‰é‡åŒ–
            z_pre = fake_quantize(z_pre, exp2_z_pre, zp_z_pre,
                                  bitwidth=self._get_bitwidth('z_pre'),
                                  symmetric=self._get_symmetric('z_pre'))

            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† sigmoid(æ¨ç†å¼•æ“ä¼šç”¨é‡åŒ–ç‰ˆæœ¬æˆ– LUT)
            z = torch.sigmoid(z_pre)

            # [ä¸ CUDA ä¸€è‡´] sigmoid è¾“å‡ºå¼ºåˆ¶ä½¿ç”¨ UINT èŒƒå›´ï¼Œå¯¹ç§°æ€§ä»é…ç½®è¯»å–
            # [ä¸ CUDA ä¸€è‡´] sigmoid è¾“å‡ºå›ºå®šä½¿ç”¨ UINT (ç¡¬ç¼–ç ï¼Œä¸å¯é…ç½®)
            z = fake_quantize(z, exp2_z_out, zp_z_out,
                              bitwidth=self._get_bitwidth('z_out'),
                              symmetric=self._get_symmetric('z_out'),
                              is_unsigned=True)

            # ========== r é—¨(Reset Gate)==========
            # [ä¸ CUDA ä¸€è‡´] r = sigmoid(Wx_r + Rh_r + bx_r + br_r)
            r_pre = Wx_r + Rh_r + bx_r.unsqueeze(0) + br_r.unsqueeze(0)

            r_pre = fake_quantize(r_pre, exp2_r_pre, zp_r_pre,
                                  bitwidth=self._get_bitwidth('r_pre'),
                                  symmetric=self._get_symmetric('r_pre'))

            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† sigmoid
            r = torch.sigmoid(r_pre)

            # [ä¸ CUDA ä¸€è‡´] sigmoid è¾“å‡ºå¼ºåˆ¶ä½¿ç”¨ UINT èŒƒå›´ï¼Œå¯¹ç§°æ€§ä»é…ç½®è¯»å–
            # [ä¸ CUDA ä¸€è‡´] sigmoid è¾“å‡ºå›ºå®šä½¿ç”¨ UINT (ç¡¬ç¼–ç ï¼Œä¸å¯é…ç½®)
            r = fake_quantize(r, exp2_r_out, zp_r_out,
                              bitwidth=self._get_bitwidth('r_out'),
                              symmetric=self._get_symmetric('r_out'),
                              is_unsigned=True)

            # ========== g é—¨(New Gate / Candidate)==========
            # [ä¸ CUDA ä¸€è‡´] g = tanh(Wx_g + r * (Rh_g + br_g) + bx_g)
            Rh_add_br = Rh_g + br_g.unsqueeze(0)

            # [ä¸ CUDA ä¸€è‡´] ä¸­é—´ç»“æœé‡åŒ–(ä»é…ç½®è¯»å–ä½å®½)
            Rh_add_br = fake_quantize(Rh_add_br, quant_params.exp2_inv_Rh_add_br_,
                                      quant_params.zp_Rh_add_br_,
                                      bitwidth=self._get_bitwidth('Rh_add_br'),
                                      symmetric=self._get_symmetric('Rh_add_br'))

            rRh = r * Rh_add_br

            # [ä¸ CUDA ä¸€è‡´] ä¹˜ç§¯é‡åŒ–(ä»é…ç½®è¯»å–ä½å®½)
            rRh = fake_quantize(rRh, quant_params.exp2_inv_rRh_,
                                quant_params.zp_rRh_,
                                bitwidth=self._get_bitwidth('rRh'),
                                symmetric=self._get_symmetric('rRh'))

            g_pre = Wx_g + rRh + bx_g.unsqueeze(0)

            g_pre = fake_quantize(g_pre, exp2_g_pre, zp_g_pre,
                                  bitwidth=self._get_bitwidth('g_pre'),
                                  symmetric=self._get_symmetric('g_pre'))

            # [ONNX å…¼å®¹] ä½¿ç”¨æ ‡å‡† tanh
            g = torch.tanh(g_pre)

            # [ä¸ CUDA ä¸€è‡´] æ¿€æ´»åé‡åŒ–ï¼Œå¯¹ç§°æ€§ä»é…ç½®è¯»å–
            g = fake_quantize(g, exp2_g_out, zp_g_out,
                              bitwidth=self._get_bitwidth('g_out'),
                              symmetric=self._get_symmetric('g_out'))

            # ========== æ–°éšè—çŠ¶æ€ ==========
            # [ä¸ CUDA ä¸€è‡´] h_new = z * h + (1 - z) * g
            # CUDA computeH åˆ†åˆ«è®¡ç®—å¹¶é‡åŒ– old_contrib å’Œ new_contrib

            # old_contrib = z * h(ä»é…ç½®è¯»å–ä½å®½)
            old_contrib = z * h
            old_contrib = fake_quantize(old_contrib, quant_params.exp2_inv_old_contrib_,
                                        quant_params.zp_old_contrib_,
                                        bitwidth=self._get_bitwidth('old_contrib'),
                                        symmetric=self._get_symmetric('old_contrib'))

            # new_contrib = (1 - z) * g(ä»é…ç½®è¯»å–ä½å®½)
            new_contrib = (1 - z) * g
            new_contrib = fake_quantize(new_contrib, quant_params.exp2_inv_new_contrib_,
                                        quant_params.zp_new_contrib_,
                                        bitwidth=self._get_bitwidth('new_contrib'),
                                        symmetric=self._get_symmetric('new_contrib'))

            # h_new = old_contrib + new_contrib
            h_new = old_contrib + new_contrib

            # [ä¸ CUDA ä¸€è‡´] è¾“å‡ºé‡åŒ–
            h_new = fake_quantize(h_new, exp2_h, zp_h,
                                  bitwidth=self._get_bitwidth('h'),
                                  symmetric=self._get_symmetric('h'))

            h = h_new

            # ä½¿ç”¨ç´¢å¼•èµ‹å€¼å­˜å‚¨(ONNX å‹å¥½)
            outputs[t] = h

        # ========== è¾“å‡º ==========
        output = outputs  # [T, B, H]ï¼Œå·²é¢„åˆ†é…
        h_n = h.unsqueeze(0)  # [1, B, H]

        return output, h_n

    def _forward_python(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        çº¯ PyTorch å®ç°çš„ GRU å‰å‘ä¼ æ’­(ç”¨äº ONNX å¯¼å‡º)

        æ”¯æŒå•å‘å’ŒåŒå‘æ¨¡å¼
        
        Note: batch_first è½¬æ¢å·²åœ¨ forward() ä¸­ç»Ÿä¸€å¤„ç†
        """
        # ===== QDQ æ¨¡å¼æå‰æ ¡éªŒ(å¿«é€Ÿå¤±è´¥)=====
        if self._export_format == 'qdq':
            if self.quant_params is None:
                raise RuntimeError(
                    "export_format='qdq' éœ€è¦é‡åŒ–å‚æ•°ï¼Œ"
                    "è¯·å…ˆè®¾ç½® calibrating=True è¿›è¡Œæ ¡å‡†"
                )
            if self.bidirectional and self.quant_params_reverse is None:
                raise RuntimeError(
                    "åŒå‘ GRU çš„ export_format='qdq' éœ€è¦åå‘é‡åŒ–å‚æ•°ï¼Œ"
                    "è¯·å…ˆè®¾ç½® calibrating=True è¿›è¡Œæ ¡å‡†"
                )

        T, B, I = input.shape

        # åˆå§‹çŠ¶æ€å¤„ç†(ç»Ÿä¸€æ¥å£)
        h0_forward, h0_reverse = self._parse_initial_state(hx, B, to_cuda=False)

        # å‰å‘æ–¹å‘
        output_forward, h_n_forward = self._forward_python_single_direction(
            input, h0_forward,
            self.weight_ih_l0, self.weight_hh_l0,
            self.bias_ih_l0 if self.bias else None,
            self.bias_hh_l0 if self.bias else None,
            self.quant_params
        )

        # åå‘æ–¹å‘(åŒå‘æ—¶)
        output_reverse, h_n_reverse = None, None
        if self.bidirectional:
            output_reverse, h_n_reverse = self._forward_python_single_direction(
                input.flip(0), h0_reverse,
                self.weight_ih_l0_reverse, self.weight_hh_l0_reverse,
                self.bias_ih_l0_reverse if self.bias else None,
                self.bias_hh_l0_reverse if self.bias else None,
                self.quant_params_reverse
            )
            # åè½¬åå‘è¾“å‡ºä»¥å¯¹é½æ—¶é—´æ­¥
            output_reverse = output_reverse.flip(0)

        # åˆå¹¶åŒå‘è¾“å‡º(ç»Ÿä¸€æ¥å£)
        return self._combine_bidirectional_outputs(
            output_forward, h_n_forward, output_reverse, h_n_reverse
        )

    # -------------------- æ ¡å‡†æ¨¡å¼ forward --------------------

    def _forward_with_calibration(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¸¦æ ¡å‡†æ•°æ®æ”¶é›†çš„å‰å‘ä¼ æ’­
        
        åœ¨ forward è¿‡ç¨‹ä¸­åŒæ—¶æ”¶é›†æ ¡å‡†æ•°æ®ï¼Œé¿å…é¢å¤–çš„å‰å‘ä¼ æ’­
        
        Note: batch_first è½¬æ¢å·²åœ¨ forward() ä¸­ç»Ÿä¸€å¤„ç†
        """
        time_steps, batch_size, input_size = input.shape
        hidden_size = self.hidden_size

        device = input.device if input.is_cuda else torch.device('cuda')
        if not input.is_cuda:
            input = input.to(device)

        # ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Š
        if not next(self.parameters()).is_cuda:
            for param in self.parameters():
                param.data = param.data.to(device)
            for buffer in self.buffers():
                buffer.data = buffer.data.to(device)

        # åˆå§‹çŠ¶æ€å¤„ç†(ç»Ÿä¸€æ¥å£)
        h0_forward, h0_reverse = self._parse_initial_state(hx, batch_size, device, to_cuda=True)

        # åˆå§‹åŒ–æ ¡å‡†æ”¶é›†å™¨(ç»Ÿä¸€æ¥å£)
        self._ensure_calibration_collectors(hidden_size, reverse=False)
        hist_collectors, quant_ranges = self._get_calibration_args(reverse=False)

        # å‡†å¤‡æƒé‡(ä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°)
        W, R, bx, br = convert_weights_to_haste_format(
            self.weight_ih_l0, self.weight_hh_l0,
            self.bias_ih_l0 if self.bias else None,
            self.bias_hh_l0 if self.bias else None,
            self.hidden_size, device
        )
        dummy_quant_params = gru_ops.GRUQuantitativeParameters()

        # å‰å‘ä¼ æ’­ + æ ¡å‡†æ•°æ®æ”¶é›†(ç»Ÿä¸€çš„ forward_interface è°ƒç”¨)
        h, v = gru_ops.forward_interface(
            is_training=True,
            is_quant=False,
            time_steps=time_steps, batch_size=batch_size,
            input_size=input_size, hidden_size=hidden_size,
            W=W, R=R, bx=bx, br=br, x=input,
            h0=h0_forward if h0_forward is not None else torch.empty(0, device=device),
            quant_params=dummy_quant_params,
            calib_method=self.calibration_method,
            hist_collectors=hist_collectors,
            quant_ranges=quant_ranges
        )

        # æå–è¾“å‡º: h å½¢çŠ¶ [T+1, B, H]ï¼Œè¾“å‡ºå– h[1:] å³ [T, B, H]
        output_forward = h[1:].contiguous()  # [T, B, H]
        h_n_forward = h[-1:].unsqueeze(0) if h.dim() == 2 else h[-1:].contiguous()  # [1, B, H]

        if self.bidirectional:
            # åˆå§‹åŒ–åå‘æ ¡å‡†æ”¶é›†å™¨(ç»Ÿä¸€æ¥å£)
            self._ensure_calibration_collectors(hidden_size, reverse=True)
            hist_collectors_rev, quant_ranges_rev = self._get_calibration_args(reverse=True)

            W_rev, R_rev, bx_rev, br_rev = convert_weights_to_haste_format(
                self.weight_ih_l0_reverse, self.weight_hh_l0_reverse,
                self.bias_ih_l0_reverse if self.bias else None,
                self.bias_hh_l0_reverse if self.bias else None,
                self.hidden_size, device
            )
            input_reversed = input.flip(0).contiguous()

            h_rev, v_rev = gru_ops.forward_interface(
                is_training=True,
                is_quant=False,
                time_steps=time_steps, batch_size=batch_size,
                input_size=input_size, hidden_size=hidden_size,
                W=W_rev, R=R_rev, bx=bx_rev, br=br_rev, x=input_reversed,
                h0=h0_reverse if h0_reverse is not None else torch.empty(0, device=device),
                quant_params=dummy_quant_params,
                calib_method=self.calibration_method,
                hist_collectors=hist_collectors_rev,
                quant_ranges=quant_ranges_rev
            )

            # æå–åå‘è¾“å‡º(å·²ç¿»è½¬)
            output_reverse = h_rev[1:].flip(0).contiguous()  # [T, B, H]
            h_n_reverse = h_rev[-1:].contiguous()  # [1, B, H]
        else:
            output_reverse, h_n_reverse = None, None

        # æ ‡è®°é‡åŒ–å‚æ•°éœ€è¦æ›´æ–°ï¼ˆæ ¡å‡†æ•°æ®å·²æ”¶é›†ï¼‰
        self._quant_params_dirty = True

        # åˆå¹¶åŒå‘è¾“å‡º(ç»Ÿä¸€æ¥å£)
        return self._combine_bidirectional_outputs(
            output_forward, h_n_forward, output_reverse, h_n_reverse
        )

    # -------------------- ä¸» forward æ–¹æ³• --------------------

    def forward(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            input: [T, B, I] æˆ– [B, T, I] (batch_first) çš„è¾“å…¥
            hx: åˆå§‹éšè—çŠ¶æ€ï¼Œå•å‘ [1, B, H]ï¼ŒåŒå‘ [2, B, H]
            
        Returns:
            output: [T, B, H] æˆ– [T, B, 2H] (åŒå‘)
            h_n: [1, B, H] æˆ– [2, B, H] (åŒå‘)

        Note:
            - export_mode=False (é»˜è®¤): ä½¿ç”¨ CUDA C++ å®ç°(é«˜æ€§èƒ½)
            - export_mode=True: ä½¿ç”¨çº¯ PyTorch å®ç°(å¯è¢« ONNX è¿½è¸ª)
        """
        # ===== ç»Ÿä¸€å¤„ç† batch_first è¾“å…¥è½¬æ¢(å”¯ä¸€å…¥å£)=====
        if self.batch_first:
            input = input.transpose(0, 1).contiguous()

        # ===== æ ¹æ®æ¨¡å¼é€‰æ‹©æ‰§è¡Œè·¯å¾„ =====
        if self.export_mode:
            # ONNX å¯¼å‡ºæ¨¡å¼ï¼šä½¿ç”¨çº¯ PyTorch å®ç°
            output, h_n = self._forward_python(input, hx)
        elif self.calibrating:
            # æ ¡å‡†æ¨¡å¼ï¼šåœ¨ forward è¿‡ç¨‹ä¸­æ”¶é›†æ ¡å‡†æ•°æ®
            self._ensure_cublas_initialized()
            output, h_n = self._forward_with_calibration(input, hx)
        else:
            # æ­£å¸¸/é‡åŒ–æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ CUDA C++ å®ç°
            self._ensure_cublas_initialized()
            output, h_n = self._forward_cuda(input, hx)

        # ===== ç»Ÿä¸€å¤„ç† batch_first è¾“å‡ºè½¬æ¢(å”¯ä¸€å‡ºå£)=====
        if self.batch_first:
            output = output.transpose(0, 1).contiguous()

        return output, h_n

    def _forward_cuda(
            self,
            input: torch.Tensor,
            hx: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        CUDA C++ å®ç°çš„å‰å‘ä¼ æ’­(æ­£å¸¸/é‡åŒ–æ¨ç†æ¨¡å¼)
        
        Note: batch_first è½¬æ¢å·²åœ¨ forward() ä¸­ç»Ÿä¸€å¤„ç†
        """
        # é‡åŒ–æ¨¡å¼ä¸‹æ£€æŸ¥æ ¡å‡†çŠ¶æ€
        if self.use_quantization:
            if self._quant_params_dirty:
                # æ ¡å‡†æ•°æ®å·²æ›´æ–°æˆ–é…ç½®å·²ä¿®æ”¹ï¼Œéœ€è¦é‡æ–°è®¡ç®—é‡åŒ–å‚æ•°
                self.finalize_calibration()
            elif not self.is_calibrated():
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„æ ¡å‡†æ•°æ®(æ”¯æŒ minmax/histogram/percentile)
                if self.quant_ranges is not None or self.hist_collectors is not None:
                    # å·²ç´¯ç§¯æ•°æ®ä½†æœªå®Œæˆæ ¡å‡†ï¼Œè‡ªåŠ¨è°ƒç”¨ finalize
                    self.finalize_calibration()
                else:
                    raise RuntimeError(
                        "é‡åŒ–å·²å¯ç”¨ä½†æœªæ ¡å‡†ã€‚è¯·å…ˆè¿›è¡Œæ ¡å‡†ï¼š\n"
                        "  1. gru.calibrating = True\n"
                        "  2. gru(calibration_data)\n"
                        "  3. gru.calibrating = False\n"
                        "æ³¨æ„ï¼špickle/deepcopy åæ ¡å‡†æ•°æ®ä¼šä¸¢å¤±ï¼Œéœ€è¦é‡æ–°æ ¡å‡†ã€‚"
                    )

        seq_len, batch_size, input_size = input.shape

        device = input.device if input.is_cuda else torch.device('cuda')
        input = ensure_cuda_float32(input, device)

        # åˆå§‹çŠ¶æ€å¤„ç†(ç»Ÿä¸€æ¥å£)
        h0_forward, h0_reverse = self._parse_initial_state(hx, batch_size, device, to_cuda=True)

        # å‰å‘æ–¹å‘
        # LUT ç°åœ¨å­˜å‚¨åœ¨ quant_params ä¸­ï¼Œé€šè¿‡ setRescaleParam å¤åˆ¶åˆ° QuantGRUReScale
        output_forward, h_n_forward = GRUFunction.apply(
            input, self.weight_ih_l0, self.weight_hh_l0,
            self.bias_ih_l0 if self.bias else None,
            self.bias_hh_l0 if self.bias else None,
            h0_forward, self.training, self.use_quantization, self.quant_params)

        # åå‘æ–¹å‘(åŒå‘æ—¶)
        output_reverse, h_n_reverse = None, None
        if self.bidirectional:
            # LUT å­˜å‚¨åœ¨ quant_params_reverse ä¸­
            output_reverse, h_n_reverse = GRUFunction.apply(
                input.flip(0), self.weight_ih_l0_reverse, self.weight_hh_l0_reverse,
                self.bias_ih_l0_reverse if self.bias else None,
                self.bias_hh_l0_reverse if self.bias else None,
                h0_reverse, self.training, self.use_quantization, self.quant_params_reverse)
            # åè½¬åå‘è¾“å‡ºä»¥å¯¹é½æ—¶é—´æ­¥
            output_reverse = output_reverse.flip(0)

        # åˆå¹¶åŒå‘è¾“å‡º(ç»Ÿä¸€æ¥å£)
        return self._combine_bidirectional_outputs(
            output_forward, h_n_forward, output_reverse, h_n_reverse
        )


# ============================================================
#                      è°ƒè¯•ä¸è¯Šæ–­å·¥å…·
# ============================================================
#
# ä»¥ä¸‹å‡½æ•°ç”¨äºè°ƒè¯•å’Œè¯Šæ–­é‡åŒ–é—®é¢˜ï¼š
#   - print_quant_params: æ‰“å°é‡åŒ–å‚æ•°ï¼ˆscale/zero_pointï¼‰
#   - print_quant_ranges: æ‰“å°æ ¡å‡†æ”¶é›†åˆ°çš„æ•°å€¼èŒƒå›´

def print_quant_params(gru: QuantGRU):
    """
    æ‰“å° QuantGRU çš„é‡åŒ–å‚æ•°

    Args:
        gru: å·²å®Œæˆæ ¡å‡†çš„ QuantGRU å®ä¾‹
    """
    if not gru.is_calibrated():
        raise RuntimeError("è¯·å…ˆè°ƒç”¨ finalize_calibration()")

    params = gru.quant_params
    print("=" * 60)
    print("GRUQuantitativeParameters (é‡åŒ–å‚æ•°)")
    print("=" * 60)
    print(f"  hidden_ = {params.hidden_}")
    print(f"  [x]  exp2_inv={params.exp2_inv_x_:3d}, zp={params.zp_x_}")
    print(f"  [h]  exp2_inv={params.exp2_inv_h_:3d}, zp={params.zp_h_}")
    print(f"  [Wx] exp2_inv={params.exp2_inv_Wx_:3d}, zp={params.zp_Wx_}")
    print(f"  [Rh] exp2_inv={params.exp2_inv_Rh_:3d}, zp={params.zp_Rh_}")
    print("-" * 60)
    print(f"  [z_pre] exp2_inv={params.exp2_inv_z_pre_:3d}, zp={params.zp_z_pre_}")
    print(f"  [r_pre] exp2_inv={params.exp2_inv_r_pre_:3d}, zp={params.zp_r_pre_}")
    print(f"  [g_pre] exp2_inv={params.exp2_inv_g_pre_:3d}, zp={params.zp_g_pre_}")
    print(f"  [z_out] exp2_inv={params.exp2_inv_z_out_:3d}, zp={params.zp_z_out_}")
    print(f"  [r_out] exp2_inv={params.exp2_inv_r_out_:3d}, zp={params.zp_r_out_}")
    print(f"  [g_out] exp2_inv={params.exp2_inv_g_out_:3d}, zp={params.zp_g_out_}")
    print("-" * 60)
    print(f"  [Rh_add_br_g]        exp2_inv={params.exp2_inv_Rh_add_br_:3d}, zp={params.zp_Rh_add_br_}")
    print(f"  [rRh]              exp2_inv={params.exp2_inv_rRh_:3d}, zp={params.zp_rRh_}")
    print(f"  [new_contrib]      exp2_inv={params.exp2_inv_new_contrib_:3d}, zp={params.zp_new_contrib_}")
    print(f"  [old_contrib]      exp2_inv={params.exp2_inv_old_contrib_:3d}, zp={params.zp_old_contrib_}")
    print("-" * 60)
    if params.exp2_inv_W_:
        print(f"  [W] exp2_inv (first 5): {list(params.exp2_inv_W_[:5])} ...")
    if params.exp2_inv_R_:
        print(f"  [R] exp2_inv (first 5): {list(params.exp2_inv_R_[:5])} ...")
    if params.exp2_inv_bx_:
        print(f"  [bx] exp2_inv (first 5): {list(params.exp2_inv_bx_[:5])} ...")
    if params.exp2_inv_br_:
        print(f"  [br] exp2_inv (first 5): {list(params.exp2_inv_br_[:5])} ...")
    print("=" * 60)


def print_quant_ranges(gru: QuantGRU):
    """
    æ‰“å° QuantGRU çš„é‡åŒ–èŒƒå›´

    Args:
        gru: å·²å®Œæˆæ ¡å‡†çš„ QuantGRU å®ä¾‹(calibrating=True åè°ƒç”¨è¿‡ forward)
    """
    if gru.quant_ranges is None:
        raise RuntimeError("è¯·å…ˆè®¾ç½® calibrating=True å¹¶è°ƒç”¨ forward()")

    r = gru.quant_ranges
    print("=" * 60)
    print("GRUQuantizationRanges (é‡åŒ–èŒƒå›´)")
    print("=" * 60)
    print(f"  hidden_ = {r.hidden_}")
    print(f"  [x]  min={r.min_x_:12.6f}, max={r.max_x_:12.6f}")
    print(f"  [h]  min={r.min_h_:12.6f}, max={r.max_h_:12.6f}")
    print(f"  [Wx] min={r.min_Wx_:12.6f}, max={r.max_Wx_:12.6f}")
    print(f"  [Rh] min={r.min_Rh_:12.6f}, max={r.max_Rh_:12.6f}")
    print("-" * 60)
    print(f"  [z_pre] min={r.min_z_pre_:12.6f}, max={r.max_z_pre_:12.6f}")
    print(f"  [r_pre] min={r.min_r_pre_:12.6f}, max={r.max_r_pre_:12.6f}")
    print(f"  [g_pre] min={r.min_g_pre_:12.6f}, max={r.max_g_pre_:12.6f}")
    print(f"  [z_out] min={r.min_z_out_:12.6f}, max={r.max_z_out_:12.6f}")
    print(f"  [r_out] min={r.min_r_out_:12.6f}, max={r.max_r_out_:12.6f}")
    print(f"  [g_out] min={r.min_g_out_:12.6f}, max={r.max_g_out_:12.6f}")
    print("-" * 60)
    print(f"  [Rh_add_br_g]        min={r.min_Rh_add_br_g_:12.6f}, max={r.max_Rh_add_br_g_:12.6f}")
    print(f"  [rRh]              min={r.min_rRh_:12.6f}, max={r.max_rRh_:12.6f}")
    print(f"  [new_contrib]      min={r.min_new_contrib_:12.6f}, max={r.max_new_contrib_:12.6f}")
    print(f"  [old_contrib]      min={r.min_old_contrib_:12.6f}, max={r.max_old_contrib_:12.6f}")
    print("=" * 60)
