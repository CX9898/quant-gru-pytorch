# GRU PyTorch - é‡åŒ–GRUå®ç°

ä¸€ä¸ªé«˜æ€§èƒ½çš„GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰PyTorchå®ç°ï¼Œæ”¯æŒé‡åŒ–å’Œéé‡åŒ–ä¸¤ç§æ¨¡å¼ï¼ŒåŸºäºCUDA/C++åç«¯ä¼˜åŒ–ã€‚

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [ä¸»è¦ç‰¹æ€§](#ä¸»è¦ç‰¹æ€§)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [å®‰è£…ä¸ç¼–è¯‘](#å®‰è£…ä¸ç¼–è¯‘)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
  - [è®­ç»ƒç¤ºä¾‹](#è®­ç»ƒç¤ºä¾‹)
  - [é‡åŒ–æ¨ç†ç¤ºä¾‹](#é‡åŒ–æ¨ç†ç¤ºä¾‹)
- [APIæ–‡æ¡£](#apiæ–‡æ¡£)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [è´¡çŒ®æŒ‡å—](#è´¡çŒ®æŒ‡å—)
- [è®¸å¯è¯](#è®¸å¯è¯)

## ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªé«˜æ€§èƒ½çš„GRUæ¨¡å—ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- **å®Œå…¨å…¼å®¹PyTorchæ¥å£**ï¼šç»§æ‰¿è‡ª`nn.GRU`ï¼Œå¯ç›´æ¥æ›¿æ¢æ ‡å‡†GRU
- **æ”¯æŒé‡åŒ–æ¨ç†**ï¼šæ”¯æŒint8å’Œint16é‡åŒ–ï¼Œå¤§å¹…é™ä½å†…å­˜å ç”¨å’Œè®¡ç®—å¼€é”€
- **CUDAåŠ é€Ÿ**ï¼šåŸºäºHASTE GRUçš„C++/CUDAå®ç°ï¼Œå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œè®¡ç®—
- **æ”¯æŒè®­ç»ƒå’Œæ¨ç†**ï¼šå®Œæ•´çš„åå‘ä¼ æ’­æ”¯æŒï¼Œå¯ç”¨äºç«¯åˆ°ç«¯è®­ç»ƒ
- **çµæ´»çš„é‡åŒ–ç­–ç•¥**ï¼šæ”¯æŒåˆ†æ®µçº¿æ€§é‡åŒ–ã€äºŒæ¬¡å¤šé¡¹å¼é‡åŒ–ç­‰å¤šç§é‡åŒ–æ–¹æ¡ˆ

## âœ¨ ä¸»è¦ç‰¹æ€§

### 1. é‡åŒ–æ”¯æŒ
- âœ… **int8é‡åŒ–**ï¼š4å€å†…å­˜å‹ç¼©ï¼Œé€‚åˆç§»åŠ¨ç«¯å’Œè¾¹ç¼˜è®¾å¤‡
- âœ… **int16é‡åŒ–**ï¼šæ›´é«˜ç²¾åº¦ï¼Œé€‚åˆå¯¹ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„åœºæ™¯
- âœ… **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰**ï¼šæ”¯æŒè®­ç»ƒæ—¶é‡åŒ–ï¼Œæå‡é‡åŒ–åç²¾åº¦
- âœ… **åŠ¨æ€é‡åŒ–**ï¼šå‰å‘ä¼ æ’­æ—¶å®æ—¶é‡åŒ–æƒé‡ï¼Œæ”¯æŒè®­ç»ƒæ—¶æƒé‡æ›´æ–°

### 2. æ€§èƒ½ä¼˜åŒ–
- âœ… **CUDAåŠ é€Ÿ**ï¼šä½¿ç”¨cuBLASè¿›è¡ŒçŸ©é˜µè¿ç®—ä¼˜åŒ–
- âœ… **å†…å­˜ä¼˜åŒ–**ï¼šé‡åŒ–æƒé‡å‡å°‘å†…å­˜å ç”¨
- âœ… **è®¡ç®—ä¼˜åŒ–**ï¼šæ•´æ•°è¿ç®—åŠ é€Ÿæ¨ç†é€Ÿåº¦

### 3. åŠŸèƒ½å®Œæ•´æ€§
- âœ… **åˆå§‹éšè—çŠ¶æ€æ”¯æŒ**ï¼šæ”¯æŒè‡ªå®šä¹‰åˆå§‹éšè—çŠ¶æ€
- âœ… **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒbatch_firstå’Œæ ‡å‡†åºåˆ—æ ¼å¼
- âœ… **æ¢¯åº¦è®¡ç®—**ï¼šå®Œæ•´çš„åå‘ä¼ æ’­æ”¯æŒ
- âœ… **è®­ç»ƒæ¨¡å¼**ï¼šæ”¯æŒè®­ç»ƒå’Œæ¨ç†ä¸¤ç§æ¨¡å¼

## ğŸ“ é¡¹ç›®ç»“æ„

```
gru-pytorch/
â”œâ”€â”€ pytouch/                  # Pythonå®ç°å’Œæ¥å£
â”‚   â”œâ”€â”€ custom_gru.py        # CustomGRUä¸»å®ç°
â”‚   â”œâ”€â”€ gru_train.py         # è®­ç»ƒç¤ºä¾‹
â”‚   â”œâ”€â”€ example_custom_gru.py # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ setup.py             # Pythonæ‰©å±•ç¼–è¯‘é…ç½®
â”‚   â””â”€â”€ lib/                 # ç¼–è¯‘åçš„åº“æ–‡ä»¶
â”œâ”€â”€ include/                  # C++å¤´æ–‡ä»¶
â”‚   â”œâ”€â”€ gru_interface.hpp   # GRUæ¥å£å®šä¹‰
â”‚   â”œâ”€â”€ gru.h                # GRUæ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ gru_quant.h          # é‡åŒ–ç›¸å…³å®šä¹‰
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/                      # C++/CUDAæºæ–‡ä»¶
â”‚   â”œâ”€â”€ gru_interface.cpp    # ç»Ÿä¸€æ¥å£
â”‚   â”œâ”€â”€ gru_forward_gpu.cu   # å‰å‘ä¼ æ’­CUDAå®ç°
â”‚   â”œâ”€â”€ gru_backward_gpu.cu  # åå‘ä¼ æ’­CUDAå®ç°
â”‚   â”œâ”€â”€ gru_forward_gpu_quant.cu # é‡åŒ–å‰å‘ä¼ æ’­CUDAå®ç°
â”‚   â””â”€â”€ quantize_ops.cu      # é‡åŒ–æ“ä½œå®ç°
â”œâ”€â”€ example/                  # C++ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ gru.cc               # GRUæµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•ç¨‹åº
â”œâ”€â”€ CMakeLists.txt           # CMakeæ„å»ºé…ç½®
â””â”€â”€ README.md                # æœ¬æ–‡ä»¶
```

## ğŸ”§ å®‰è£…ä¸ç¼–è¯‘

### å‰ç½®è¦æ±‚

- **Python**: >= 3.7
- **PyTorch**: >= 1.8.0 (æ”¯æŒCUDA)
- **CUDA**: >= 10.0
- **CMake**: >= 3.18
- **C++ç¼–è¯‘å™¨**: æ”¯æŒC++17
- **cuBLAS**: CUDAå·¥å…·åŒ…çš„ä¸€éƒ¨åˆ†

### ç¼–è¯‘æ­¥éª¤

#### 1. ç¼–è¯‘C++/CUDAåº“

```bash
# åˆ›å»ºæ„å»ºç›®å½•
mkdir -p build && cd build

# é…ç½®CMake
cmake ..

# ç¼–è¯‘
make -j$(nproc)

# åº“æ–‡ä»¶å°†è¾“å‡ºåˆ° pytorch/lib/ ç›®å½•
```

#### 2. ç¼–è¯‘Pythonæ‰©å±•

```bash
cd pytorch

# ç¼–è¯‘Pythonæ‰©å±•ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
python setup.py build_ext --inplace

# æˆ–è€…å®‰è£…ä¸ºåŒ…
python setup.py install
```

### éªŒè¯å®‰è£…

```python
import torch
from custom_gru import CustomGRU

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
assert torch.cuda.is_available(), "éœ€è¦CUDAæ”¯æŒ"

# åˆ›å»ºæ¨¡å‹æµ‹è¯•
gru = CustomGRU(input_size=128, hidden_size=256, use_quantization=False).cuda()
print("âœ… å®‰è£…æˆåŠŸï¼")
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### éé‡åŒ–æ¨¡å¼

```python
import torch
from custom_gru import CustomGRU

# åˆ›å»ºæ¨¡å‹
gru = CustomGRU(
    input_size=128,
    hidden_size=256,
    batch_first=True,
    use_quantization=False
).cuda()

# å‰å‘ä¼ æ’­
x = torch.randn(4, 100, 128).cuda()  # [batch, seq_len, input_size]
output, h_n = gru(x)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [4, 100, 256]
print(f"éšè—çŠ¶æ€å½¢çŠ¶: {h_n.shape}")  # [1, 4, 256]
```

### é‡åŒ–æ¨¡å¼ï¼ˆint8ï¼‰

```python
import torch
from custom_gru import CustomGRU

# å‡†å¤‡æ ¡å‡†æ•°æ®ï¼ˆç”¨äºé‡åŒ–å‚æ•°æ ¡å‡†ï¼‰
calibration_data = torch.randn(4, 100, 128).cuda()

# åˆ›å»ºé‡åŒ–æ¨¡å‹
gru = CustomGRU(
    input_size=128,
    hidden_size=256,
    batch_first=True,
    use_quantization=True,
    quant_type='int8',
    calibration_data=calibration_data
).cuda()

# å‰å‘ä¼ æ’­
x = torch.randn(4, 100, 128).cuda()
output, h_n = gru(x)
```

### ä»PyTorch GRUè¿ç§»

```python
import torch
import torch.nn as nn
from custom_gru import CustomGRU

# åŸå§‹PyTorch GRU
pytorch_gru = nn.GRU(input_size=128, hidden_size=256, batch_first=True).cuda()

# åˆ›å»ºCustomGRU
custom_gru = CustomGRU(
    input_size=128,
    hidden_size=256,
    batch_first=True,
    use_quantization=False
).cuda()

# å¤åˆ¶æƒé‡
with torch.no_grad():
    custom_gru.weight_ih_l0.copy_(pytorch_gru.weight_ih_l0)
    custom_gru.weight_hh_l0.copy_(pytorch_gru.weight_hh_l0)
    custom_gru.bias_ih_l0.copy_(pytorch_gru.bias_ih_l0)
    custom_gru.bias_hh_l0.copy_(pytorch_gru.bias_hh_l0)

# éªŒè¯è¾“å‡ºä¸€è‡´æ€§
x = torch.randn(4, 100, 128).cuda()
with torch.no_grad():
    out1, h1 = pytorch_gru(x)
    out2, h2 = custom_gru(x)
    print(f"è¾“å‡ºå·®å¼‚: {torch.max(torch.abs(out1 - out2)).item():.6f}")
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### è®­ç»ƒç¤ºä¾‹

```python
import torch
import torch.nn as nn
from custom_gru import CustomGRU

# å®šä¹‰æ¨¡å‹
class GRUNet(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super().__init__()
        self.gru = CustomGRU(
            input_size=input_size,
            hidden_size=hidden_size,
            batch_first=True,
            use_quantization=False  # è®­ç»ƒæ—¶é€šå¸¸ä¸ä½¿ç”¨é‡åŒ–
        )
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        out, _ = self.gru(x)
        out = out[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        return self.fc(out)

# åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
model = GRUNet(input_size=128, hidden_size=256, num_classes=10).cuda()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# è®­ç»ƒå¾ªç¯
for epoch in range(10):
    model.train()
    for x, y in train_loader:
        x, y = x.cuda(), y.cuda()
        
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
```

### é‡åŒ–æ¨ç†ç¤ºä¾‹

```python
# 1. è®­ç»ƒæµ®ç‚¹æ¨¡å‹ï¼ˆä½¿ç”¨éé‡åŒ–æ¨¡å¼ï¼‰
model = train_float_model()

# 2. å‡†å¤‡æ ¡å‡†æ•°æ®
calibration_data = get_calibration_samples()

# 3. åˆ›å»ºé‡åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡
quant_model = CustomGRU(
    input_size=128,
    hidden_size=256,
    batch_first=True,
    use_quantization=True,
    quant_type='int8',
    calibration_data=calibration_data
).cuda()

# åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
quant_model.load_state_dict(model.state_dict())

# 4. æ¨ç†
quant_model.eval()
with torch.no_grad():
    output, h_n = quant_model(test_input)
```

## ğŸ“š APIæ–‡æ¡£

### CustomGRU

ç»§æ‰¿è‡ª`torch.nn.GRU`çš„è‡ªå®šä¹‰GRUç±»ã€‚

#### å‚æ•°

- `input_size` (int): è¾“å…¥ç‰¹å¾ç»´åº¦
- `hidden_size` (int): éšè—çŠ¶æ€ç»´åº¦
- `num_layers` (int, default=1): GRUå±‚æ•°ï¼ˆç›®å‰ä»…æ”¯æŒ1å±‚ï¼‰
- `bias` (bool, default=True): æ˜¯å¦ä½¿ç”¨åç½®
- `batch_first` (bool, default=False): å¦‚æœä¸ºTrueï¼Œè¾“å…¥å½¢çŠ¶ä¸º[batch, seq, feature]
- `dropout` (float, default=0.0): å±‚é—´dropoutæ¦‚ç‡ï¼ˆç›®å‰ä¸æ”¯æŒï¼‰
- `bidirectional` (bool, default=False): æ˜¯å¦åŒå‘ï¼ˆç›®å‰ä¸æ”¯æŒï¼‰
- `use_quantization` (bool, default=False): æ˜¯å¦ä½¿ç”¨é‡åŒ–
- `quant_type` (str, default='int8'): é‡åŒ–ç±»å‹ï¼Œ'int8' æˆ– 'int16'
- `calibration_data` (torch.Tensor, optional): ç”¨äºæ ¡å‡†é‡åŒ–å‚æ•°çš„è¾“å…¥æ•°æ®
  - å½¢çŠ¶: `[seq_len, batch, input_size]` æˆ– `[batch, seq_len, input_size]`ï¼ˆå–å†³äºbatch_firstï¼‰

#### æ–¹æ³•

- `forward(input, hx=None)`: å‰å‘ä¼ æ’­
  - `input`: è¾“å…¥å¼ é‡
  - `hx`: åˆå§‹éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º`[num_layers, batch, hidden_size]`
  - è¿”å›: `(output, h_n)`
    - `output`: è¾“å‡ºåºåˆ—ï¼Œå½¢çŠ¶ä¸inputç›¸åŒä½†æœ€åä¸€ç»´ä¸ºhidden_size
    - `h_n`: æœ€ç»ˆéšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º`[num_layers, batch, hidden_size]`

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### é‡åŒ–å®ç°

æœ¬é¡¹ç›®å®ç°äº†å¤šç§é‡åŒ–ç­–ç•¥ï¼š

1. **æƒé‡é‡åŒ–**ï¼šå°†æµ®ç‚¹æƒé‡é‡åŒ–ä¸ºint8æˆ–int16æ•´æ•°
2. **æ¿€æ´»é‡åŒ–**ï¼šä½¿ç”¨åˆ†æ®µçº¿æ€§é‡åŒ–æˆ–äºŒæ¬¡å¤šé¡¹å¼é‡åŒ–
3. **åŠ¨æ€é‡åŒ–**ï¼šå‰å‘ä¼ æ’­æ—¶å®æ—¶é‡åŒ–ï¼Œæ”¯æŒè®­ç»ƒæ—¶æƒé‡æ›´æ–°

### æ ¼å¼è½¬æ¢

- **PyTorchæ ¼å¼**: æƒé‡é¡ºåºä¸º (r, z, n) - é‡ç½®é—¨ã€æ›´æ–°é—¨ã€æ–°é—¨
- **HASTEæ ¼å¼**: æƒé‡é¡ºåºä¸º (z, r, n) - æ›´æ–°é—¨ã€é‡ç½®é—¨ã€æ–°é—¨
- è‡ªåŠ¨å¤„ç†ä¸¤ç§æ ¼å¼ä¹‹é—´çš„è½¬æ¢

### åå‘ä¼ æ’­

- ä½¿ç”¨`torch.autograd.Function`å®ç°è‡ªå®šä¹‰åå‘ä¼ æ’­
- æ”¯æŒå®Œæ•´çš„æ¢¯åº¦è®¡ç®—ï¼Œå¯ç”¨äºç«¯åˆ°ç«¯è®­ç»ƒ
- åå‘ä¼ æ’­ç»Ÿä¸€ä½¿ç”¨float32æƒé‡ï¼Œä¿è¯ç²¾åº¦

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–

- é‡åŒ–æƒé‡ï¼šint8é‡åŒ–å¯å‡å°‘75%å†…å­˜å ç”¨
- å…±äº«ä¸­é—´ç»“æœï¼šé¿å…é‡å¤åˆ†é…å†…å­˜

### è®¡ç®—ä¼˜åŒ–

- CUDAå¹¶è¡Œè®¡ç®—ï¼šå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œèƒ½åŠ›
- cuBLASä¼˜åŒ–ï¼šä½¿ç”¨ä¼˜åŒ–çš„çŸ©é˜µè¿ç®—åº“
- æ•´æ•°è¿ç®—ï¼šé‡åŒ–åä½¿ç”¨æ•´æ•°è¿ç®—åŠ é€Ÿ

## â“ å¸¸è§é—®é¢˜

### Q1: ç¼–è¯‘æ—¶å‡ºç°CUDAç›¸å…³é”™è¯¯ï¼Ÿ

**A**: ç¡®ä¿ï¼š
1. CUDAå·¥å…·åŒ…å·²æ­£ç¡®å®‰è£…
2. CMakeèƒ½å¤Ÿæ‰¾åˆ°CUDAï¼ˆæ£€æŸ¥`CMAKE_CUDA_COMPILER`ï¼‰
3. GPUæ¶æ„åŒ¹é…ï¼ˆåœ¨`setup.py`ä¸­è°ƒæ•´`-arch=sm_XX`ï¼‰

### Q2: é‡åŒ–åç²¾åº¦ä¸‹é™æ˜æ˜¾ï¼Ÿ

**A**: å°è¯•ï¼š
1. ä½¿ç”¨int16é‡åŒ–ï¼ˆæ›´é«˜ç²¾åº¦ï¼‰
2. å¢åŠ æ ¡å‡†æ•°æ®é‡
3. ä½¿ç”¨é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰

### Q3: è®­ç»ƒæ—¶æ¢¯åº¦ä¸ºNoneï¼Ÿ

**A**: ç¡®ä¿ï¼š
1. æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼ï¼ˆ`model.train()`ï¼‰
2. è¾“å…¥å¼ é‡è®¾ç½®äº†`requires_grad=True`
3. ä½¿ç”¨æ”¯æŒé‡åŒ–çš„ç‰ˆæœ¬ï¼ˆå·²å®ç°åå‘ä¼ æ’­ï¼‰

### Q4: å¦‚ä½•é€‰æ‹©é‡åŒ–ç±»å‹ï¼Ÿ

**A**: 
- **int8**: é€‚åˆå†…å­˜å—é™åœºæ™¯ï¼Œç²¾åº¦æŸå¤±è¾ƒå¤§
- **int16**: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ï¼Œæ¨èç”¨äºå¤§å¤šæ•°åœºæ™¯

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Forkæœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

### ä»£ç è§„èŒƒ

- éµå¾ªPEP 8ï¼ˆPythonä»£ç ï¼‰
- ä½¿ç”¨clang-formatæ ¼å¼åŒ–C++ä»£ç 
- æ·»åŠ é€‚å½“çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ã€‚è¯¦è§LICENSEæ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- åŸºäº[HASTE GRU](https://github.com/lmnt-com/haste)å®ç°
- ä½¿ç”¨PyTorchå’ŒCUDAè¿›è¡ŒåŠ é€Ÿ

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤Issueæˆ–Pull Requestã€‚

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ï¼ŒAPIå¯èƒ½ä¼šæœ‰å˜åŒ–ã€‚å»ºè®®æŸ¥çœ‹æœ€æ–°æ–‡æ¡£å’Œç¤ºä¾‹ä»£ç ã€‚
